{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yb6iTu3YW25O"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import math\n",
        "import json\n",
        "import sys\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import csv\n",
        "\n",
        "\n",
        "def load(file_name):\n",
        "    # data(list of list): [[index, dimensions], [.., ..], ...]\n",
        "    data = []\n",
        "    fh = open(file_name)\n",
        "    for line in fh:\n",
        "        line = line.strip().split(',')\n",
        "        temp = [int(line[0])]\n",
        "        for feature in line[1:]:\n",
        "            temp.append(float(feature))\n",
        "        data.append(temp)\n",
        "    data=np.array(data)\n",
        "    return data\n",
        "\n",
        "def get_sample(data):\n",
        "    length = len(data)\n",
        "    sample_size = int(length * 0.01)\n",
        "    random_nums = set()\n",
        "    sample_data = []\n",
        "\n",
        "    for i in range(sample_size):\n",
        "        random_index = random.randint(0, length - 1)\n",
        "        while random_index in random_nums:\n",
        "            random_index = random.randint(0, length - 1)\n",
        "        random_nums.add(random_index)\n",
        "        sample_data.append(data[random_index])\n",
        "    sample_data = np.array(sample_data)\n",
        "    return sample_data\n",
        "\n",
        "def initialize_centroids(data, dimension, k):\n",
        "    centroids = [[0 for _ in range(dimension)] for _ in range(k)]\n",
        "    max_feature_vals = [0 for _ in range(dimension)]\n",
        "    min_feature_vals = [float('inf') for _ in range(dimension)]\n",
        "    data=data[:,1:]\n",
        "    # TO DO\n",
        "    # Calculate max feature and min feture value for each dimension\n",
        "    for i in range(dimension):\n",
        "        dim=data[i]\n",
        "        min_feature_vals.append(np.amean(dim))\n",
        "        max_feature_vals.append(np.amax(dim))\n",
        "    #diff: max - min for each dimension\n",
        "    diff=max_feature_vals-min_feature_vals\n",
        "    # for each centroid, in each dimension assign centroids[j][i] = min_feature_val + diff * random.uniform(1e-5, 1)\n",
        "    for i in range(len(centroids)):\n",
        "        for j in range(len(centroids[i])):\n",
        "            centroids[j][i]=min_feature_vals+diff*random.uniform(1*np.exp(-5),1)\n",
        "    return centroids\n",
        "\n",
        "def initialize_centroids_simple(data, dimension, k):\n",
        "\n",
        "    #centroids: [(centroid0 fearures); (centroid0 features); ... ..]\n",
        "    centroids =np.array( [[0 for _ in range(dimension)] for _ in range(k)])\n",
        "    #TO DO\n",
        "    #Write your code to return initialized centroids by randomly assiging them to K points\n",
        "    row,column=data.shape\n",
        "    randidx = np.random.permutation(row)\n",
        "\n",
        "    for i in range(k):\n",
        "        centroids[i]=data[randidx[i],1:]\n",
        "\n",
        "    return centroids\n",
        "\n",
        "def get_euclidean_distance(p1, p2):\n",
        "    distance = -1.0\n",
        "    #Write your code\n",
        "    point=p2[1:]\n",
        "    distance=np.linalg.norm(p1-point)\n",
        "    return distance\n",
        "\n",
        "def kmeans(data, dimension, k):\n",
        "    #centroids: [(centroid0 fearures); (centroid1 features); ... ..]\n",
        "    centroids = initialize_centroids_simple(data, dimension, k)\n",
        "    #centroids= initialize_centroids(data,dimension,k)\n",
        "    #cluster_affiliation: [((point1index  features),clusterindex); ((point2index features), clusterindex)... ]\n",
        "    cluster_affiliation = [[tuple(features), None] for features in data]\n",
        "    flag = 1\n",
        "    j=0\n",
        "    # count=0\n",
        "    # while count!=4:\n",
        "    #     count+=1\n",
        "    while flag:\n",
        "        for i, point in enumerate(data):\n",
        "            min_distance = float('inf')\n",
        "            min_distance_index = None\n",
        "\n",
        "            #find closest centroids for each data points\n",
        "            for cluster_index, centroid in enumerate(centroids):\n",
        "                if centroid[0] == None:\n",
        "                    continue\n",
        "                distance = get_euclidean_distance(centroid, point)\n",
        "                if distance < min_distance:\n",
        "                    min_distance = distance\n",
        "                    min_distance_index = cluster_index\n",
        "\n",
        "            #record or update cluster for each data points\n",
        "            if cluster_affiliation[i][1] != min_distance_index:\n",
        "               cluster_affiliation[i][1] = min_distance_index\n",
        "\n",
        "        #recompute centroids\n",
        "        centroids = np.array([[0 for _ in range(dimension)] for _ in range(k)])\n",
        "\n",
        "        clutser_point_count = np.array([0 for _ in range(k)])\n",
        "        #TO DO\n",
        "        #write your code to count each cluster pointcount and store them in clutser_point_count structure\n",
        "        #recompute centroids using the count\n",
        "\n",
        "        for i in range(len(cluster_affiliation)):\n",
        "            if cluster_affiliation[i][1]==0:\n",
        "                clutser_point_count[0]+=1\n",
        "                point=np.array(data[i,1:])\n",
        "                centroids[0]=centroids[0]+point\n",
        "            elif cluster_affiliation[i][1]==1:\n",
        "                clutser_point_count[1]+=1\n",
        "                point=np.array(data[i,1:])\n",
        "                centroids[1]=centroids[1]+point\n",
        "            elif cluster_affiliation[i][1]==2:\n",
        "                clutser_point_count[2]+=1\n",
        "                point=np.array(data[i,1:])\n",
        "                centroids[2]=centroid[2]+point\n",
        "            else:\n",
        "                clutser_point_count[3]+=1\n",
        "                point=np.array(data[i,1:])\n",
        "                centroids[3]=centroids[3]+point\n",
        "\n",
        "\n",
        "        for i in range(k):\n",
        "            centroids[i]=centroids[i]/clutser_point_count[i]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    #TO DO\n",
        "    #Terminate the while loop based on termination criteria. Write your code to turn flag = false\n",
        "\n",
        "\n",
        "    for i in data:\n",
        "        min=i-centroids[0]\n",
        "        for d in range(k):\n",
        "            store=i-centroid[d]\n",
        "            store=(np.linalg.norm(store)**2)\n",
        "            if(min>store):\n",
        "                min=store\n",
        "        j+=min\n",
        "        if(i==data[0,:]):\n",
        "            prev=j\n",
        "        # print(j,\" ,\" ,prev)\n",
        "        a=abs(j-prev)\n",
        "        b=((10**-5)*j)\n",
        "        if(a<=b):\n",
        "            flag=0\n",
        "\n",
        "\n",
        "    return (centroids, cluster_affiliation)\n",
        "\n",
        "\n",
        "def main():\n",
        "    start = time.time()\n",
        "\n",
        "    #input path of the real data\n",
        "    #data file contains point index and 50 features in each line separated by comma\n",
        "    inputpath =  'F://data'\n",
        "    K = 4 # K clusters\n",
        "    output1 = 'out1.csv'\n",
        "    output2 = 'out2.csv'\n",
        "\n",
        "    data_num = 0\n",
        "    data = load(inputpath + '/data' + str(data_num) + '.txt')\n",
        "\n",
        "    dimension = len(data[0]) - 1\n",
        "\n",
        "    #sampling data from the data file\n",
        "    sample_data = get_sample(data)\n",
        "\n",
        "    centroids, cluster_affiliation = kmeans(sample_data, dimension, K)\n",
        "    print(centroids.shape)\n",
        "    f=open('F://data/out1.csv','w')\n",
        "    writer = csv.writer(f)\n",
        "    for i in centroids:\n",
        "        writer.writerow(i)\n",
        "    f.close()\n",
        "\n",
        "    f=open('F://data/out2.csv','w')\n",
        "    writer = csv.writer(f)\n",
        "    for i in cluster_affiliation:\n",
        "        writer.writerow(i)\n",
        "    f.close()\n",
        "if __name__ == \"__main__\":\n",
        "\t\t\t\tmain()\n",
        ""
      ]
    }
  ]
}