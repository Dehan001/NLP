{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ztu3XmCseDkr",
        "outputId": "0891e821-5c66-479b-ee7f-09911d330900"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.8 MB 5.3 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.10.0\n",
            "  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
            "\u001b[K     |████████████████████████████████| 182 kB 59.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 43.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (3.0.4)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.11.1 tokenizers-0.13.2 transformers-4.25.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60uJZ0zKRzQU"
      },
      "source": [
        "Reddit dataset classification using transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vda_bu7QyaIy"
      },
      "source": [
        "Importing the libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PSExBhLvSKWq",
        "outputId": "7090e6e5-4e25-45e6-f317-e9719487f1b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers.pipelines import TextClassificationPipeline\n",
        "import tensorflow.keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import InputLayer\n",
        "from tensorflow.keras.layers import Dense\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kQmvUHcP808t"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import MultiHeadAttention, LayerNormalization, Dropout, Layer\n",
        "from tensorflow.keras.layers import Embedding, Input, GlobalAveragePooling1D, Dense\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R641b0e2ygZs"
      },
      "source": [
        "1st function: selecting 10 words from each comments. The comments without atleast 10 words have eos by default."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zb9Ym7LPTqtf"
      },
      "outputs": [],
      "source": [
        "def tenwords(str):\n",
        "  tokens=str.split()[:10]\n",
        "  str1=''\n",
        "\n",
        "  for i in range(len(tokens)):\n",
        "    if i==0:\n",
        "      str1+=tokens[i]\n",
        "    else:\n",
        "\n",
        "      str1=str1+\" \"+tokens[i]\n",
        "  return str1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8gMzF7w0y0mQ"
      },
      "source": [
        "2nd function: as part of the preprossesing I am removing cleaning punctuation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SWzsC4wSu3qL"
      },
      "outputs": [],
      "source": [
        "def cleaningPunc(str):\n",
        "  str=re.sub(r'[^\\w\\s]','',str)\n",
        "  return str"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qotP05bhu4jb"
      },
      "source": [
        "33d function: as part of the preprossesing I am removing the urls."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1DUTH_FllThx"
      },
      "outputs": [],
      "source": [
        "def urlremove(sentence):\n",
        "  sentence=re.sub(r'http\\S+', '', sentence)\n",
        "  sentence=re.sub(r'www\\S+', '', sentence)\n",
        "  return sentence\n",
        "\n",
        "\n",
        "\n",
        "  #sentence=re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '',sentence)\n",
        "  #sentence = re.sub('@[^\\s]+','',sentence)\n",
        "  #setence = re.sub('http[^\\s]+','',sentence)\n",
        "  #sentence=re.sub('https:\\/\\/.*','',sentence)\n",
        "\n",
        "  #sentence=re.sub(r'^https?:\\/\\/.*[\\r\\n]*','',sentence)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCoFJMGjy95g"
      },
      "source": [
        "4th function: as part preprocessing removing stopwords like[I,is....]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t7r-GCUc83ei"
      },
      "outputs": [],
      "source": [
        "def stopwordsRemove(str):\n",
        "  str1=\"\"\n",
        "  stop_words=stopwords.words('english')\n",
        "  str=str.split()\n",
        "\n",
        "  for word in str:\n",
        "    if not word in stop_words:\n",
        "      str1+=word+' '\n",
        "  return str1\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rO6jLLnzIwx"
      },
      "source": [
        "5th function: as part of the preprossesing removing digits like 1,2..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PjVIFBWUKnjY"
      },
      "outputs": [],
      "source": [
        "def cleaningdigits(str):\n",
        "  str=re.sub(r'[\\d+]','',str)\n",
        "  return str"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4iiMJvv40VNf"
      },
      "source": [
        "6th function: stemming but according to the vocabulary ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kgnKhDJLmVSu"
      },
      "outputs": [],
      "source": [
        "def lemmatization(str):\n",
        "  lemma=WordNetLemmatizer()\n",
        "  str1=''\n",
        "  tokens=str.split()\n",
        "  for word in tokens:\n",
        "    store=lemma.lemmatize(word)\n",
        "    str1+=store+' '\n",
        "  return str1\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f2vCvR-nRvSQ"
      },
      "outputs": [],
      "source": [
        "#reading the csv file\n",
        "dataFrame=pd.read_csv(\"reddit_data.csv\")\n",
        "#dataFrame.head()\n",
        "\n",
        "\n",
        "#removing the columns that is not necessary now.\n",
        "dataFrame.drop('parent_id',inplace=True,axis=1)\n",
        "dataFrame.drop('length',inplace=True,axis=1)\n",
        "dataFrame.drop('size_range',inplace=True,axis=1)\n",
        "\n",
        "\n",
        "#dataFrame.head()\n",
        "\n",
        "#removing duplicates from the dataset\n",
        "dataFrame=dataFrame.drop_duplicates()\n",
        "\n",
        "#removing float values(Nan) from the dataset\n",
        "dataFrame=dataFrame[dataFrame['text'].notnull()]\n",
        "dataFrame=dataFrame[dataFrame['topic'].notnull()]\n",
        "#dataFrame['text'] = dataFrame['text'].apply(lambda x: re.split('https:\\/\\/.*', str(x))[0])\n",
        "#spliting the dataset\n",
        "train,test=train_test_split(dataFrame,test_size=0.3)\n",
        "\n",
        "#separating the x_train data and y_train data. Converting them into numpy array.\n",
        "x_train=train['text'].to_numpy()\n",
        "y_train=train['topic'].to_numpy()\n",
        "x_test=test['text'].to_numpy()\n",
        "y_test=test['topic'].to_numpy()\n",
        "\n",
        "x_test=x_test[:10]\n",
        "y_test=y_test[:10]\n",
        "x_train=x_train[:10]\n",
        "y_train=y_train[:10]\n",
        "\n",
        "#preprocessing [taking 10 words,cleaning punctuation, stopwords Remove,cleaningdightis, lemmatization ]\n",
        "count=0\n",
        "for i in range(len(x_train)):\n",
        "  sentence=x_train[i]\n",
        "\n",
        "  sentence=tenwords(sentence)\n",
        "\n",
        "  sentence=cleaningPunc(sentence)\n",
        "\n",
        "  sentence=stopwordsRemove(sentence)\n",
        "\n",
        "  sentence=cleaningdigits(sentence)\n",
        "\n",
        "  sentence=lemmatization(sentence)\n",
        "\n",
        "  sentence=urlremove(sentence)\n",
        "  x_train[i]=sentence\n",
        "\n",
        "\n",
        "\n",
        "for i in range(len(x_test)):\n",
        "  sentence=x_test[i]\n",
        "\n",
        "  sentence=tenwords(sentence)\n",
        "\n",
        "  sentence=cleaningPunc(sentence)\n",
        "\n",
        "  sentence=stopwordsRemove(sentence)\n",
        "\n",
        "  sentence=cleaningdigits(sentence)\n",
        "\n",
        "  sentence=lemmatization(sentence)\n",
        "\n",
        "  sentence=urlremove(sentence)\n",
        "  x_test[i]=sentence\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HCrwojNgGv7w"
      },
      "outputs": [],
      "source": [
        "#one hot encoding\n",
        "token_index={}\n",
        "counter=0\n",
        "for sentence in x_train:\n",
        "  tokens=sentence.split()\n",
        "  for token in tokens:\n",
        "    if not token in token_index:\n",
        "      token_index.update({token:counter+1})\n",
        "      counter=counter+1\n",
        "\n",
        "#print(token_index)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0vYJN4G_b_Qg"
      },
      "outputs": [],
      "source": [
        "#checking vocabulary size and indices\n",
        "#unique_words=token_index.keys()\n",
        "#nique_values=token_index.values()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "26kzMbbDzUr2",
        "outputId": "71d0392b-b10b-4b0d-bc2a-952a9ca359c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10, 10, 53)\n",
            "(10, 10, 53)\n"
          ]
        }
      ],
      "source": [
        "# Set max_length to 10 representaing 10 words\n",
        "max_length = 10\n",
        "# Create a tensor of dimension 3 named results whose every elements are initialized to 0\n",
        "results  = np.zeros(shape = (len(x_train),\n",
        "                            max_length,\n",
        "                            max(token_index.values()) + 1))\n",
        "print(results.shape)\n",
        "\n",
        "results_test  = np.zeros(shape = (len(x_test),\n",
        "                            max_length,\n",
        "                            max(token_index.values()) + 1))\n",
        "\n",
        "\n",
        "print(results_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# One-hot vector corresponding to the word\n",
        "# Iterate over enumerate(samples) enumerate object\n",
        "for i, documents in enumerate(x_train):\n",
        "\n",
        "# Convert enumerate object to list and iterate over resultant list\n",
        "  for j, considered_word in list(enumerate(documents.split())):\n",
        "\n",
        "    # set the value of index variable equal to the value of considered_word in token_index\n",
        "    index = token_index.get(considered_word)\n",
        "\n",
        "\n",
        "    # In the previous zero tensor: results, set the value of elements with their positional index as [i, j, index] = 1.\n",
        "    results[i, j, index] = 1.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4aCLN756uZwR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# One-hot vector corresponding to the word\n",
        "# Iterate over enumerate(samples) enumerate object\n",
        "for i, documents in enumerate(x_test):\n",
        "\n",
        "# Convert enumerate object to list and iterate over resultant list\n",
        "  for j, considered_word in list(enumerate(documents.split())):\n",
        "\n",
        "    # set the value of index variable equal to the value of considered_word in token_index\n",
        "    index = token_index.get(considered_word)\n",
        "\n",
        "\n",
        "    # In the previous zero tensor: results, set the value of elements with their positional index as [i, j, index] = 1.\n",
        "    results_test[i, j, index] = 1.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "KY3YyCH5ntXg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementing text classification on reddit data"
      ],
      "metadata": {
        "id": "lEcpkZ3maHD5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.att = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.ffn = Sequential(\n",
        "            [Dense(ff_dim, activation=\"relu\"),\n",
        "             Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = Dropout(rate)\n",
        "        self.dropout2 = Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        attn_output = self.att(inputs, inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)"
      ],
      "metadata": {
        "id": "tFSZGuAh-pEN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TokenAndPositionEmbedding(Layer):\n",
        "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
        "        super(TokenAndPositionEmbedding, self).__init__()\n",
        "        self.token_emb = Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
        "        self.pos_emb = Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
        "\n",
        "    def call(self, x):\n",
        "        maxlen = tf.shape(x)[-1]\n",
        "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
        "        positions = self.pos_emb(positions)\n",
        "        x = self.token_emb(x)\n",
        "        return x + positions"
      ],
      "metadata": {
        "id": "uLsgCU5O-sXK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#vocab_size = 20000  # Only consider the top 20k words\n",
        "#maxlen = 200  # Only consider the first 200 words of each movie review\n",
        "\n",
        "#(x_train, y_train), (x_val, y_val) = imdb.load_data(num_words=vocab_size)\n",
        "#print(len(x_train), \"Training sequences\")\n",
        "#print(len(x_val), \"Validation sequences\")\n",
        "y_train_dict={}\n",
        "counter=0\n",
        "for i,label in enumerate(y_train):\n",
        "  if not label in y_train_dict:\n",
        "      y_train_dict.update({label:counter+1})\n",
        "      counter=counter+1\n",
        "  y_train[i]=y_train_dict[label]\n",
        "\n",
        "for i,label in enumerate(y_test):\n",
        "  if not label in y_train_dict:\n",
        "    y_train_dict.update({label:counter+1})\n",
        "    counter=counter+1\n",
        "  y_test[i]=y_train_dict[label]\n"
      ],
      "metadata": {
        "id": "-VZQlXcR-tov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "maxlen=10\n",
        "vocab_size=len(token_index)\n",
        "#x_train = tf.keras.preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "#y_train= tf.keras.preprocessing.sequence.pad_sequences(y_train, maxlen=maxlen)\n",
        "\n",
        "#x_val = tf.keras.preprocessing.sequence.pad_sequences(x_val, maxlen=maxlen)"
      ],
      "metadata": {
        "id": "rK7PQrM7-ySC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_dim = 32  # Embedding size for each token\n",
        "num_heads = 1  # Number of attention heads\n",
        "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
        "\n",
        "inputs = Input(shape=(maxlen,))\n",
        "embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
        "x = embedding_layer(inputs)\n",
        "transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
        "x = transformer_block(x)\n",
        "x = GlobalAveragePooling1D()(x)\n",
        "x = Dropout(0.1)(x)\n",
        "x = Dense(20, activation=\"relu\")(x)\n",
        "x = Dropout(0.1)(x)\n",
        "outputs = Dense(2, activation=\"softmax\")(x)\n",
        "\n",
        "model = Model(inputs=inputs, outputs=outputs)"
      ],
      "metadata": {
        "id": "IW6pqIDv-3GH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "x_train=np.asarray(results).astype(np.int)\n",
        "\n",
        "y_train=np.asarray(y_train).astype(np.int)\n",
        "x_test=np.asarray(results_test).astype(np.int)\n",
        "\n",
        "y_test=np.asarray(y_test).astype(np.int)\n",
        "history = model.fit(x_train, y_train,\n",
        "                    batch_size=128, epochs=2,\n",
        "                    validation_data=(results_test, y_test)\n",
        "                   )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3JEenpkf-5SV",
        "outputId": "330b4afc-dcdd-40d0-ddc9-a87159816439"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-128-a509144ac480>:2: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  x_train=np.asarray(results).astype(np.int)\n",
            "<ipython-input-128-a509144ac480>:4: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  y_train=np.asarray(y_train).astype(np.int)\n",
            "<ipython-input-128-a509144ac480>:5: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  x_test=np.asarray(results_test).astype(np.int)\n",
            "<ipython-input-128-a509144ac480>:7: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  y_test=np.asarray(y_test).astype(np.int)\n",
            "WARNING:tensorflow:Model was constructed with shape (None, 10) for input KerasTensor(type_spec=TensorSpec(shape=(None, 10), dtype=tf.float32, name='input_8'), name='input_8', description=\"created by layer 'input_8'\"), but it was called on an input with incompatible shape (None, 10, 53).\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-128-a509144ac480>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m history = model.fit(x_train, y_train, \n\u001b[0m\u001b[1;32m      9\u001b[0m                     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/__autograph_generated_filec9mj3qvl.py\u001b[0m in \u001b[0;36mtf__call\u001b[0;34m(self, inputs, training)\u001b[0m\n\u001b[1;32m      8\u001b[0m                 \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                 \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUndefinedReturnValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m                 \u001b[0mattn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m                 \u001b[0mattn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                 \u001b[0mout1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayernorm1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1051, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1040, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1030, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 889, in train_step\n        y_pred = self(x, training=True)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/tmp/__autograph_generated_filec9mj3qvl.py\", line 10, in tf__call\n        attn_output = ag__.converted_call(ag__.ld(self).att, (ag__.ld(inputs), ag__.ld(inputs)), None, fscope)\n\n    ValueError: Exception encountered when calling layer \"transformer_block_7\" (type TransformerBlock).\n    \n    in user code:\n    \n        File \"<ipython-input-50-f7ee72c50368>\", line 15, in call  *\n            attn_output = self.att(inputs, inputs)\n        File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 67, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n    \n        ValueError: Exception encountered when calling layer \"query\" (type EinsumDense).\n        \n        Shape must be rank 3 but is rank 4\n        \t for 0th input and equation: abc,cde->abde for '{{node model_7/transformer_block_7/multi_head_attention_7/query/einsum/Einsum}} = Einsum[N=2, T=DT_FLOAT, equation=\"abc,cde->abde\"](model_7/token_and_position_embedding_7/add, model_7/transformer_block_7/multi_head_attention_7/query/einsum/Einsum/ReadVariableOp)' with input shapes: [?,10,53,32], [32,1,32].\n        \n        Call arguments received by layer \"query\" (type EinsumDense):\n          • inputs=tf.Tensor(shape=(None, 10, 53, 32), dtype=float32)\n    \n    \n    Call arguments received by layer \"transformer_block_7\" (type TransformerBlock):\n      • inputs=tf.Tensor(shape=(None, 10, 53, 32), dtype=float32)\n      • training=True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = model.evaluate(x_test, y_test, verbose=2)\n",
        "\n",
        "for name, value in zip(model.metrics_names, results):\n",
        "    print(\"%s: %.3f\" % (name, value))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "eOH1J-Q8-HmS",
        "outputId": "01014727-93a4-4b8a-acad-cf920876aee0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Model was constructed with shape (None, 10) for input KerasTensor(type_spec=TensorSpec(shape=(None, 10), dtype=tf.float32, name='input_8'), name='input_8', description=\"created by layer 'input_8'\"), but it was called on an input with incompatible shape (None, 10, 53).\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-130-df7bf64dd4f2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s: %.3f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtf__test_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/__autograph_generated_filec9mj3qvl.py\u001b[0m in \u001b[0;36mtf__call\u001b[0;34m(self, inputs, training)\u001b[0m\n\u001b[1;32m      8\u001b[0m                 \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                 \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUndefinedReturnValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m                 \u001b[0mattn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m                 \u001b[0mattn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                 \u001b[0mout1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayernorm1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1557, in test_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1546, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1535, in run_step  **\n        outputs = model.test_step(data)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1499, in test_step\n        y_pred = self(x, training=False)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/tmp/__autograph_generated_filec9mj3qvl.py\", line 10, in tf__call\n        attn_output = ag__.converted_call(ag__.ld(self).att, (ag__.ld(inputs), ag__.ld(inputs)), None, fscope)\n\n    ValueError: Exception encountered when calling layer \"transformer_block_7\" (type TransformerBlock).\n    \n    in user code:\n    \n        File \"<ipython-input-50-f7ee72c50368>\", line 15, in call  *\n            attn_output = self.att(inputs, inputs)\n        File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 67, in error_handler  **\n            raise e.with_traceback(filtered_tb) from None\n    \n        ValueError: Exception encountered when calling layer \"query\" (type EinsumDense).\n        \n        Shape must be rank 3 but is rank 4\n        \t for 0th input and equation: abc,cde->abde for '{{node model_7/transformer_block_7/multi_head_attention_7/query/einsum/Einsum}} = Einsum[N=2, T=DT_FLOAT, equation=\"abc,cde->abde\"](model_7/token_and_position_embedding_7/add, model_7/transformer_block_7/multi_head_attention_7/query/einsum/Einsum/ReadVariableOp)' with input shapes: [?,10,53,32], [32,1,32].\n        \n        Call arguments received by layer \"query\" (type EinsumDense):\n          • inputs=tf.Tensor(shape=(None, 10, 53, 32), dtype=float32)\n    \n    \n    Call arguments received by layer \"transformer_block_7\" (type TransformerBlock):\n      • inputs=tf.Tensor(shape=(None, 10, 53, 32), dtype=float32)\n      • training=False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Implementing contextiual word and then mlp**\n",
        "\n"
      ],
      "metadata": {
        "id": "ORKSGdn1fTks"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bert***\n"
      ],
      "metadata": {
        "id": "ohh_Lp7IpQdZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MLP = Sequential()\n",
        "MLP.add(InputLayer(input_shape=(720, ))) # input layer not complete\n",
        "MLP.add(Dense(256, activation='relu')) # hidden layer 1\n",
        "MLP.add(Dense(256, activation='relu')) # hidden layer 2\n",
        "MLP.add(Dense(10, activation='softmax')) # output layer\n",
        "\n",
        "# summary\n",
        "MLP.summary()"
      ],
      "metadata": {
        "id": "3ymacfUfi_9K",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 131
        },
        "outputId": "c4c05b5a-d85f-4585-f9a1-58797dd66830"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-d0b4c2147d02>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    MLP.add(InputLayer(input_shape=(, ))) # input layer not complete\u001b[0m\n\u001b[0m                                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JRcdCueYCsom",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "outputId": "2eb3324c-0b0a-4d4f-d0ad-d3b1eaafea6a"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-102239395f32>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m MLP.compile(loss='categorical_crossentropy',\n\u001b[0m\u001b[1;32m      2\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m             metrics=['accuracy'])\n\u001b[1;32m      4\u001b[0m \u001b[0mMLP\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'MLP' is not defined"
          ]
        }
      ],
      "source": [
        "MLP.compile(loss='categorical_crossentropy',\n",
        "            optimizer='adam',\n",
        "            metrics=['accuracy'])\n",
        "MLP.fit(train, train, epochs=20, batch_size=128)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# evaluate performance\n",
        "test_loss, test_acc = MLP.evaluate(train, test,batch_size=128,verbose=0)\n",
        "print(\"Test loss:\", test_loss)\n",
        "print(\"Test accuracy:\", test_acc)\n",
        "\n",
        "index=np.argmax(MLP.predict(digit, verbose=0))"
      ],
      "metadata": {
        "id": "yMLUvDctr9k0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "0bb23a1e-c2c7-4342-c5c0-ff0c56ea88e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-18fd0bf2ff2d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# evaluate performance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMLP\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Test loss:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Test accuracy:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'MLP' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Transformer implementation from scratch**"
      ],
      "metadata": {
        "id": "n0o9TKr5BSvI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self,embed_size,heads):\n",
        "        super(SelfAttention,self).__init__()\n",
        "        self.embed_size=embed_size\n",
        "        self.heads=heads\n",
        "        self.head_dim=embed_size//heads\n",
        "\n",
        "        assert (self.head_dim*heads==embed_size),\"Embed size needs to be div by heads\"\n",
        "\n",
        "        self.values=nn.Linear(self.head_dim,self.head_dim,bias=False)\n",
        "        self.keys=nn.Linear(self.head_dim,self.head_dim,bias=False)\n",
        "        self.queries=nn.Linear(self.head_dim,self.head_dim,bias=False)\n",
        "\n",
        "        self.fc_out=nn.Linear(heads*self.head_dim,embed_size)\n",
        "\n",
        "    def forward(self,values,keys,query,mask):\n",
        "        N=query.shape[0]\n",
        "        value_len,key_len,query_len=values.shape[1],keys.shape[1],query.shape[1]\n",
        "\n",
        "        values=values.reshape(N,value_len,self.heads,self.head_dim)\n",
        "\n",
        "        keys=keys.reshape(N,key_len,self.head,self.head_dim)\n",
        "\n",
        "        queries=query.reshape(N,query_len,self.heads,self.head_dim)\n",
        "        values=self.values(values)\n",
        "        key=self.keys(keys)\n",
        "        queries-self.queries(queries)\n",
        "        energy=torch.einsum(\"nqhd,nkhd->nhqk\",[queries,keys])\n",
        "\n",
        "        if mask is not None:\n",
        "            energy=energy.masked_fill(mask==0,float(\"-1e20\"))\n",
        "        attention=torch.softmax(energy/(self.embed_size**(1/2)),dim=3)\n",
        "        out=torch.einsum(\"nhql,nlhd->nqhd\",[attention,values]).reshape(N,query_len,self.heads*self.head_dim)\n",
        "\n",
        "        out=self.fc_out(out)\n",
        "        return out\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self,embed_size,heads,dropout,forward_expansion):\n",
        "        super(TransformerBlock,self).__init__()\n",
        "        self.attention=SelfAttention(embed_size,heads)\n",
        "        self.norm1=nn.LayerNorm(embed_size)\n",
        "        self.norm2=nn.LayerNorm(embed_size)\n",
        "\n",
        "        self.feed_forward=nn.Sequential(nn.Linear(embed_size,forward_expansion*embed_size),nn.Relu(),nn.Linear(forward_expansion*embed_size,embed_size))\n",
        "        self.dropout=nn.Dropout(dropout)\n",
        "    def forward(self,value,key,query,mask):\n",
        "        attention=self.attention(value,key,query,mask)\n",
        "        x=self.dropout(self.norm1(attention+query))\n",
        "        forward=self.feed_forward(x)\n",
        "        out=self.dropout(self.norm2(forward+x))\n",
        "        return out\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init(self,src_vocab_size,embed_size,num_layers,heads,device,forward_expansion,dropout,max_length):\n",
        "        super(Encoder,self).__init__()\n",
        "        self.embed_size=embed_size\n",
        "        self.device=device\n",
        "        self.word_embedding=nn.Embedding(src_vocab_size,embed_size)\n",
        "        self.postional_embedding=nn.Embedding(max_length,embed_size)\n",
        "        self.layers=nn.ModuleList([TransformerBlock(embed_size,heads,dropout=dropout,forward_expansion=forward_expansion)] for _ in range(num_layers))\n",
        "        self.dropout=nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self,x,mask):\n",
        "        N,seq_length=x.shape\n",
        "        positions=torch.arrange(0,seq_length).expand(N,seq_length).to(self.device)\n",
        "        out=self.dropout(self.word_embedding(x)+self.postional_embedding(positions))\n",
        "        for layer in self.layers:\n",
        "            out=layer(out,out,out,mask)\n",
        "        return out\n",
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self,embed_size,heads,forward_expansion,dropout,device):\n",
        "        super(DecoderBlock,self).__init__()\n",
        "        self.attention=SelfAttention(embed_size,heads)\n",
        "        self.norm=nn.LayerNorm(embed_size)\n",
        "        self.transformer_block=TransformerBlock(embed_size,heads,dropout,forward_expansion)\n",
        "        self.dropout=nn.Dropout(dropout)\n",
        "    def forward(self,x,value,key,src_mask,trg_mask):\n",
        "        attention=self.attention(x,x,x,trg_mask)\n",
        "        query=self.dropout(self.norm(attention+x))\n",
        "        out=self.transformer_block(value,key,query,src_mask)\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self,trg_vocab_size,embed_size,num_layers,heads,forward_expansion,dropout,device,max_length):\n",
        "        super(Decoder,self).__init__()\n",
        "        self.device=device\n",
        "        self.word_embedding=nn.Embedding(trg_vocab_size,embed_size)\n",
        "        self.layers=nn.ModuleList([DecoderBlock(embed_size,heads,forward_expansion,dropout,device)\n",
        "        for _ in range(num_layers)])\n",
        "        self.fc_out=nn.Linear(embed_size,trg_vocab_size)\n",
        "        self.dropout=nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self,enc_out,src_mask,trg_mask):\n",
        "        N,seq_length= x.shape\n",
        "        positions=torch.arrange(0,seq_length).expand(N,seq_length).to(self.device)\n",
        "        x=self.dropout(self.word_embedding(x)+self.postion_embedding(positions))\n",
        "        for layer in self.layers:\n",
        "            x=layer(x,enc_out,enc_out,src_mask,trg_mask)\n",
        "        out=self.fc_out(x)\n",
        "        return out\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self,src_vocab_size,trg_vocab_size,src_pad_idx,trg_pad_idx,embed_size=256,num_layers=6,forward_expansion=4,heads=8,dropout=8,device='colab',max_length=10):\n",
        "        super (Transformer,self).__init__()\n",
        "        self.encoder=Encoder(src_vocab_size,embed_size,num_layers,heads,device,forward_expansion,dropout,max_length)\n",
        "        self.decoder=Decoder(trg_vocab_size,embed_size,num_layers,heads,forward_expansion,dropout,device,max_length)\n",
        "        self.src_pad_idx=src_pad_idx\n",
        "        self.trg_pad_idx=trg_pad_idx\n",
        "    def make_src_mask(self,src):\n",
        "        src_mask=(src!=self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "        return src_mask.to(self.device)\n",
        "    def make_trg_mask(self,trg):\n",
        "        N,trg_len=trg.shape\n",
        "        trg_maks=torch.tril(torch.one((trg_len,trg_len))).expand(N,1,trg_len,trg_len)\n",
        "        return trg_mask.to(self.device)\n",
        "    def forward(self,src,trg):\n",
        "        src_mask=self.make_src_mask(src)\n",
        "        trg_mask=self.make_trg_mask(trg)\n",
        "        enc_src=self.encoder(src,src_mask)\n",
        "        out=self.decoder(trg.enc_sr,src_mask,trg_mask)\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "sPZSkZbvE1BY",
        "outputId": "40426856-c711-4eb6-eabb-a8d0e4197a09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-d2b4310a5e9a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mSelfAttention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0membed_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mheads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSelfAttention\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membed_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheads\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def getPositionEncoding(seq_len, d, n=10000):\n",
        "    P = np.zeros((seq_len, d))\n",
        "    for k in range(seq_len):\n",
        "        for i in np.arange(int(d/2)):\n",
        "            denominator = np.power(n, 2*i/d)\n",
        "            P[k, 2*i] = np.sin(k/denominator)\n",
        "            P[k, 2*i+1] = np.cos(k/denominator)\n",
        "    return P"
      ],
      "metadata": {
        "id": "JVKndwB8NWe8"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}