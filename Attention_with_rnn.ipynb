{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gc4KXi2IMxHK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f89bcd3-30ac-4ba2-a22e-342cd19958e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader,Dataset\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import gensim\n",
        "import gensim.downloader\n",
        "from sklearn.metrics import accuracy_score\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text preprocessing"
      ],
      "metadata": {
        "id": "nCjqK4jfBsUk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Textpreprocessing:\n",
        "  def __init__(self,text):\n",
        "    self.text=text\n",
        "\n",
        "  def words(self,str1):\n",
        "    tokens=str1.split()[:4]\n",
        "    str2=''\n",
        "\n",
        "    for i in range(len(tokens)):\n",
        "      if i==0:\n",
        "        str2+=tokens[i]\n",
        "      else:\n",
        "        str2=str2+\" \"+tokens[i]\n",
        "\n",
        "    return str2\n",
        "\n",
        "\n",
        "\n",
        "  def stopword_remove(self,str1):\n",
        "    str1=str1.lower()\n",
        "    tokens=str1.split()\n",
        "    str2=\"\"\n",
        "    stop_words=stopwords.words('english')\n",
        "    for word in tokens:\n",
        "      if not word in stop_words:\n",
        "        str2+=word+' '\n",
        "    return str2\n",
        "\n",
        "  def url_remove(self,str1):\n",
        "    str1=re.sub(r'http\\S+', '', str1)\n",
        "    str1=re.sub(r'www\\S+', '', str1)\n",
        "    return str1\n",
        "\n",
        "  def clean_punctuation(self,str1):\n",
        "    str1=re.sub(r'[^\\w\\s]','',str1)\n",
        "    return str1\n",
        "\n",
        "  def cleaningdigits(self,str1):\n",
        "    str1=re.sub(r'[\\d+]','',str1)\n",
        "    return str1\n",
        "\n",
        "  def lemmatization(self,str1):\n",
        "    lemma=WordNetLemmatizer()\n",
        "    str2=''\n",
        "    tokens=str1.split()\n",
        "    for word in tokens:\n",
        "      store=lemma.lemmatize(word)\n",
        "      str2+=store+' '\n",
        "    return str2\n",
        "\n",
        "  def preprocess(self):\n",
        "    self.text=self.text.apply(self.words)\n",
        "    self.text=self.text.apply(self.stopword_remove)\n",
        "    self.text=self.text.apply(self.url_remove)\n",
        "    self.text=self.text.apply(self.clean_punctuation)\n",
        "    self.text=self.text.apply(self.cleaningdigits)\n",
        "    self.text=self.text.apply(self.lemmatization)\n",
        "\n",
        "    return self.text"
      ],
      "metadata": {
        "id": "fBAOs_kr8dOn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_to_number(label):\n",
        "  if label=='spam':\n",
        "    return 1\n",
        "  return 0"
      ],
      "metadata": {
        "id": "y0Pf8yqIBhcW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url='https://drive.google.com/file/d/1xEyhcHjrjEo62k84kKu6mI9vttGtc-jV/view?usp=share_link'\n",
        "url='https://drive.google.com/uc?id=' + url.split('/')[-2]\n",
        "df = pd.read_csv(url)\n",
        "df=df.drop_duplicates()\n",
        "df=df[df['Message'].notnull()]\n",
        "df=df[df['Category'].notnull()]\n",
        "\n",
        "print(df.head)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KqEnWpiTBx2y",
        "outputId": "69bc0888-4048-4513-8abe-052606818ce7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bound method NDFrame.head of      Category                                            Message\n",
            "0         ham  Go until jurong point, crazy.. Available only ...\n",
            "1         ham                      Ok lar... Joking wif u oni...\n",
            "2        spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
            "3         ham  U dun say so early hor... U c already then say...\n",
            "4         ham  Nah I don't think he goes to usf, he lives aro...\n",
            "...       ...                                                ...\n",
            "5567     spam  This is the 2nd time we have tried 2 contact u...\n",
            "5568      ham               Will Ã¼ b going to esplanade fr home?\n",
            "5569      ham  Pity, * was in mood for that. So...any other s...\n",
            "5570      ham  The guy did some bitching but I acted like i'd...\n",
            "5571      ham                         Rofl. Its true to its name\n",
            "\n",
            "[5157 rows x 2 columns]>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_preprocess=Textpreprocessing(df['Message'])\n",
        "df['Message']=text_preprocess.preprocess()\n",
        "document_size=len(df)\n",
        "\n"
      ],
      "metadata": {
        "id": "Nftlq37GB5Ql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Numeric_label']=df['Category'].apply(convert_to_number)\n"
      ],
      "metadata": {
        "id": "HMySdN_mCHJP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_model=gensim.downloader.load('glove-twitter-25')\n",
        "new_model.most_similar(\"bad\")\n",
        "wv_size=len(new_model['bad'])"
      ],
      "metadata": {
        "id": "5ps9tb4wCJWH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_word2vec_for_setence(text,max_word):\n",
        "\n",
        "  results  = np.zeros(shape = (max_word,wv_size))\n",
        "  #for i, documents in enumerate(text):\n",
        "  for j, considered_word in list(enumerate(text.split())):\n",
        "    try:\n",
        "      results[j, :] = new_model[considered_word]\n",
        "    except:\n",
        "      results[j,:]=np.zeros((wv_size))\n",
        "  return torch.tensor(results,dtype=torch.float32)"
      ],
      "metadata": {
        "id": "0nBcU-cPCPn4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyDataset(Dataset):\n",
        "  def __init__(self,df):\n",
        "    super().__init__()\n",
        "    self.texts=df['Message'].values\n",
        "    self.labels=df['Numeric_label'].values\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.texts)\n",
        "\n",
        "  def __getitem__(self,idx):\n",
        "    text=self.texts[idx]\n",
        "    label=self.labels[idx]\n",
        "\n",
        "    word2vec= create_word2vec_for_setence(text,4)\n",
        "    label=torch.tensor(label,dtype=torch.float32)\n",
        "\n",
        "    return word2vec,label"
      ],
      "metadata": {
        "id": "1yfrEM3rEHEK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset,test_dataset=train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "train_my_dataset=MyDataset(train_dataset)\n",
        "test_my_dataset=MyDataset(test_dataset)\n",
        "train_dataloader=DataLoader(train_my_dataset,batch_size=32,shuffle=True)\n",
        "test_dataloader=DataLoader(test_my_dataset,batch_size=32,shuffle=False)"
      ],
      "metadata": {
        "id": "pRmY8nEUEMgR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyModel2(nn.Module):\n",
        "  def __init__(self,input_dim=25,hidden_dim1=64,hidden_dim2=128,hidden_dim3=64,num_layers=1):\n",
        "    super().__init__()\n",
        "    self.input_dim=input_dim\n",
        "    self.hidden_dim1=hidden_dim1\n",
        "    self.hidden_dim2=hidden_dim2\n",
        "    self.hidden_dim3=hidden_dim3\n",
        "    self.num_layers=num_layers=1\n",
        "    self.output_dim=1\n",
        "\n",
        "    self.rnn = nn.RNN(self.input_dim, self.hidden_dim1, num_layers, batch_first=True)\n",
        "\n",
        "    self.attention_layer=nn.Linear(self.hidden_dim1,1)\n",
        "\n",
        "    self.linear_layer1=nn.Linear(self.hidden_dim1,self.hidden_dim2)\n",
        "    self.linear_layer2=nn.Linear(self.hidden_dim2,self.hidden_dim3)\n",
        "    self.final_layer=nn.Linear(self.hidden_dim3,self.output_dim)\n",
        "\n",
        "    self.sigmoid=nn.Sigmoid()\n",
        "    self.softmax=nn.Softmax(dim=1)\n",
        "\n",
        "  def forward(self,x):\n",
        "    batch_size=x.shape[0]\n",
        "    h0 = torch.randn(self.num_layers, batch_size,self.hidden_dim1)\n",
        "    output, hn = self.rnn(x,h0)\n",
        "\n",
        "    #hn = hn.permute(1, 0, 2).contiguous()\n",
        "    #print(\"hn shape :\",hn.shape)\n",
        "\n",
        "    attention_score=self.attention_layer(output)\n",
        "    weights=self.softmax(attention_score)\n",
        "    weighted_outputs=output*weights\n",
        "    context_vector=torch.sum(weighted_outputs,dim=1)\n",
        "\n",
        "    #print(\"context_vector: \",context_vector.shape)\n",
        "\n",
        "\n",
        "\n",
        "    x=context_vector\n",
        "\n",
        "    #x=torch.mean(x,1)\n",
        "    #print(\"x: \",x.shape)\n",
        "    #x.requires_grad=True\n",
        "\n",
        "    #x=x.view(batch_size,-1)\n",
        "    #x=torch.conacte()\n",
        "    x=self.linear_layer1(x)\n",
        "    x=self.sigmoid(x)\n",
        "    x=self.linear_layer2(x)\n",
        "    x=self.sigmoid(x)\n",
        "    x=self.final_layer(x)\n",
        "    #print('requires_grad',x.requires_grad)\n",
        "    x=self.sigmoid(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "    #return x.view(batch_size, self.output_dim)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gXR-2fua5o9J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model2=MyModel2()"
      ],
      "metadata": {
        "id": "SmfqpLBR9KYJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer=torch.optim.Adam(model2.parameters(),lr=0.001)"
      ],
      "metadata": {
        "id": "7jsRD6XM3hTH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion=nn.BCELoss(reduction='mean')"
      ],
      "metadata": {
        "id": "rYMNs6VD3hTC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(10):\n",
        "  overall_accuracy=0\n",
        "  for x,y in train_dataloader:\n",
        "    batch_size=x.shape[0]\n",
        "\n",
        "    predicted_y=model2(x)\n",
        "\n",
        "    y=y.view(batch_size,1)\n",
        "\n",
        "    loss=criterion(predicted_y,y)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    y_true=y.detach().numpy()\n",
        "\n",
        "    y_pred=predicted_y.detach().numpy() >0.5\n",
        "\n",
        "\n",
        "\n",
        "    accuracy= accuracy_score(y_true,y_pred)\n",
        "    overall_accuracy +=accuracy*batch_size\n",
        "\n",
        "\n",
        "  print(f'Epoch: {epoch} --> Accuracy {overall_accuracy/len(train_my_dataset)}')\n",
        "\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GGqOeL4j8-1E",
        "outputId": "03897a48-2448-46d4-f0e3-5d2f1d3b75b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 --> Accuracy 0.8722424242424243\n",
            "Epoch: 1 --> Accuracy 0.8858181818181818\n",
            "Epoch: 2 --> Accuracy 0.904\n",
            "Epoch: 3 --> Accuracy 0.9098181818181819\n",
            "Epoch: 4 --> Accuracy 0.9052121212121212\n",
            "Epoch: 5 --> Accuracy 0.9081212121212121\n",
            "Epoch: 6 --> Accuracy 0.9122424242424242\n",
            "Epoch: 7 --> Accuracy 0.9086060606060606\n",
            "Epoch: 8 --> Accuracy 0.9117575757575758\n",
            "Epoch: 9 --> Accuracy 0.9112727272727272\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(10):\n",
        "  overall_accuracy=0\n",
        "  for x,y in test_dataloader:\n",
        "    batch_size=x.shape[0]\n",
        "\n",
        "    predicted_y=model2(x)\n",
        "\n",
        "    y=y.view(batch_size,1)\n",
        "\n",
        "    loss=criterion(predicted_y,y)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    y_true=y.detach().numpy()\n",
        "    y_pred=predicted_y.detach().numpy() >0.5\n",
        "\n",
        "\n",
        "\n",
        "    accuracy= accuracy_score(y_true,y_pred)\n",
        "    overall_accuracy +=accuracy*batch_size\n",
        "\n",
        "\n",
        "  print(f'Epoch: {epoch} --> Accuracy {overall_accuracy/len(test_my_dataset)}')"
      ],
      "metadata": {
        "id": "7shPNEf29B7c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ceaef30-6d46-4514-86ee-b516c5f81320"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 --> Accuracy 0.8972868217054264\n",
            "Epoch: 1 --> Accuracy 0.9011627906976745\n",
            "Epoch: 2 --> Accuracy 0.9031007751937985\n",
            "Epoch: 3 --> Accuracy 0.9011627906976745\n",
            "Epoch: 4 --> Accuracy 0.9069767441860465\n",
            "Epoch: 5 --> Accuracy 0.9069767441860465\n",
            "Epoch: 6 --> Accuracy 0.9098837209302325\n",
            "Epoch: 7 --> Accuracy 0.9060077519379846\n",
            "Epoch: 8 --> Accuracy 0.9108527131782945\n",
            "Epoch: 9 --> Accuracy 0.9118217054263565\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nuauSRu3Wr2I"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}