{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader,Dataset\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import gensim\n",
        "import gensim.downloader"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3SnzBphafVx_",
        "outputId": "11723011-f015-4ccd-a801-77c0e9851b86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Textpreprocessing:\n",
        "  def __init__(self,text):\n",
        "    self.text=text\n",
        "\n",
        "  def words(self,str1):\n",
        "    tokens=str1.split()[:4]\n",
        "    str2=''\n",
        "\n",
        "    for i in range(len(tokens)):\n",
        "      if i==0:\n",
        "        str2+=tokens[i]\n",
        "      else:\n",
        "        str2=str2+\" \"+tokens[i]\n",
        "\n",
        "    return str2\n",
        "\n",
        "\n",
        "\n",
        "  def stopword_remove(self,str1):\n",
        "    str1=str1.lower()\n",
        "    tokens=str1.split()\n",
        "    str2=\"\"\n",
        "    stop_words=stopwords.words('english')\n",
        "    for word in tokens:\n",
        "      if not word in stop_words:\n",
        "        str2+=word+' '\n",
        "    return str2\n",
        "\n",
        "  def url_remove(self,str1):\n",
        "    str1=re.sub(r'http\\S+', '', str1)\n",
        "    str1=re.sub(r'www\\S+', '', str1)\n",
        "    return str1\n",
        "\n",
        "  def clean_punctuation(self,str1):\n",
        "    str1=re.sub(r'[^\\w\\s]','',str1)\n",
        "    return str1\n",
        "\n",
        "  def cleaningdigits(self,str1):\n",
        "    str1=re.sub(r'[\\d+]','',str1)\n",
        "    return str1\n",
        "\n",
        "  def lemmatization(self,str1):\n",
        "    lemma=WordNetLemmatizer()\n",
        "    str2=''\n",
        "    tokens=str1.split()\n",
        "    for word in tokens:\n",
        "      store=lemma.lemmatize(word)\n",
        "      str2+=store+' '\n",
        "    return str2\n",
        "\n",
        "  def preprocess(self):\n",
        "    self.text=self.text.apply(self.words)\n",
        "    self.text=self.text.apply(self.stopword_remove)\n",
        "    self.text=self.text.apply(self.url_remove)\n",
        "    self.text=self.text.apply(self.clean_punctuation)\n",
        "    self.text=self.text.apply(self.cleaningdigits)\n",
        "    self.text=self.text.apply(self.lemmatization)\n",
        "\n",
        "    return self.text"
      ],
      "metadata": {
        "id": "MTmWilHbUVAQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_to_number(label):\n",
        "  if label=='spam':\n",
        "    return 1\n",
        "  return 0"
      ],
      "metadata": {
        "id": "_q4uzRetArUX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url='https://drive.google.com/file/d/1xEyhcHjrjEo62k84kKu6mI9vttGtc-jV/view?usp=share_link'\n",
        "url='https://drive.google.com/uc?id=' + url.split('/')[-2]\n",
        "df = pd.read_csv(url)\n",
        "df=df.drop_duplicates()\n",
        "df=df[df['Message'].notnull()]\n",
        "df=df[df['Category'].notnull()]\n",
        "\n",
        "print(df.head)"
      ],
      "metadata": {
        "id": "a58dwFRBixDF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "outputId": "73c213d0-7f41-47b5-f15c-1b6cb5a37237"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-2c46c6f43be8>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'https://drive.google.com/file/d/1xEyhcHjrjEo62k84kKu6mI9vttGtc-jV/view?usp=share_link'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'https://drive.google.com/uc?id='\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_duplicates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Message'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_preprocess=Textpreprocessing(df['Message'])\n",
        "df['Message']=text_preprocess.preprocess()\n",
        "document_size=len(df)\n",
        "\n"
      ],
      "metadata": {
        "id": "KeyjyziLZoLz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "outputId": "492829f6-0059-4276-9342-3d38f447805c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-a5601ed351b1>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtext_preprocess\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTextpreprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Message'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Message'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_preprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdocument_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Textpreprocessing' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['Numeric_label']=df['Category'].apply(convert_to_number)\n"
      ],
      "metadata": {
        "id": "s2dlmepcUpLi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#change glove overall\n",
        "new_model=gensim.downloader.load('glove-twitter-25')\n",
        "new_model.most_similar(\"bad\")\n",
        "wv_size=len(new_model['bad'])"
      ],
      "metadata": {
        "id": "w3EacCy4TGuW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_word2vec_for_setence(text,max_word):\n",
        "\n",
        "  results  = np.zeros(shape = (max_word,wv_size))\n",
        "  #for i, documents in enumerate(text):\n",
        "  for j, considered_word in list(enumerate(text.split())):\n",
        "    try:\n",
        "      results[j, :] = new_model[considered_word]\n",
        "    except:\n",
        "      results[j,:]=np.zeros((wv_size))\n",
        "  return torch.tensor(results,dtype=torch.float32)"
      ],
      "metadata": {
        "id": "5pPsHts4ZzzA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyDataset(Dataset):\n",
        "  def __init__(self,df):\n",
        "    super().__init__()\n",
        "    self.texts=df['Message'].values\n",
        "    self.labels=df['Numeric_label'].values\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.texts)\n",
        "\n",
        "  def __getitem__(self,idx):\n",
        "    text=self.texts[idx]\n",
        "    label=self.labels[idx]\n",
        "\n",
        "    word2vec= create_word2vec_for_setence(text,4)\n",
        "    label=torch.tensor(label,dtype=torch.float32)\n",
        "\n",
        "    return word2vec,label\n",
        "\n"
      ],
      "metadata": {
        "id": "wrH2qggIUxtK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "outputId": "572176fb-aeb2-44b0-ad2d-8a01c9a61e5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-0e16b8e42fce>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mMyDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Message'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Numeric_label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Dataset' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset,test_dataset=train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "train_my_dataset=MyDataset(train_dataset)\n",
        "test_my_dataset=MyDataset(test_dataset)\n",
        "train_dataloader=DataLoader(train_my_dataset,batch_size=32,shuffle=True)\n",
        "test_dataloader=DataLoader(test_my_dataset,batch_size=32,shuffle=False)"
      ],
      "metadata": {
        "id": "ck1z1JQaWXvO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#update input_dim and hidden_dim\n",
        "#why requires_grad=False\n",
        "class MyModel(nn.Module):\n",
        "  def __init__(self,input_dim=25,hidden_dim=10):\n",
        "    super().__init__()\n",
        "    self.input_dim=input_dim\n",
        "    self.hidden_dim=hidden_dim\n",
        "    self.linear_layer1=nn.Linear(self.input_dim,self.hidden_dim)\n",
        "    self.linear_layer2=nn.Linear(self.hidden_dim,self.hidden_dim)\n",
        "    self.final_layer=nn.Linear(self.hidden_dim,1)\n",
        "    self.sigmoid=nn.Sigmoid()\n",
        "\n",
        "  def forward(self,x):\n",
        "    print(x.shape)\n",
        "\n",
        "    x=torch.mean(x,1)\n",
        "    print(x.shape)\n",
        "    x.requires_grad=True\n",
        "\n",
        "    #x=x.view(batch_size,-1)\n",
        "    #x=torch.conacte()\n",
        "    x=self.linear_layer1(x)\n",
        "    x=self.sigmoid(x)\n",
        "    x=self.linear_layer2(x)\n",
        "    x=self.sigmoid(x)\n",
        "    x=self.final_layer(x)\n",
        "    #print('requires_grad',x.requires_grad)\n",
        "    x=self.sigmoid(x)\n",
        "\n",
        "\n",
        "    return x\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wURpuFK6VJxZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=MyModel()"
      ],
      "metadata": {
        "id": "gfRXxGliX_Wh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion=nn.BCELoss(reduction='mean')"
      ],
      "metadata": {
        "id": "67HWsTYGYkcT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        },
        "outputId": "cdd5c604-bbfd-4781-c343-523b5e1f6c3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-b5315b620b89>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcriterion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBCELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mean'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer=torch.optim.Adam(model.parameters(),lr=0.0001)"
      ],
      "metadata": {
        "id": "kYuSSlZ0YpGX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        },
        "outputId": "87939b64-4a8e-4d51-d29a-97025aeede03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-f07b157a3b18>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "Ixfj6kPhynO5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for epoch in range(10):\n",
        "  overall_accuracy=0\n",
        "  for x,y in train_dataloader:\n",
        "    predicted_y=model(x)\n",
        "\n",
        "    batch_size=x.shape[0]\n",
        "    y=y.view(batch_size,1)\n",
        "\n",
        "    loss=criterion(predicted_y,y)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "    y_true=y.detach().numpy()\n",
        "\n",
        "    y_pred=predicted_y.detach().numpy() >0.5\n",
        "\n",
        "\n",
        "\n",
        "    accuracy= accuracy_score(y_true,y_pred)\n",
        "\n",
        "    overall_accuracy +=accuracy*batch_size\n",
        "\n",
        "\n",
        "  print(f'Epoch: {epoch} --> Accuracy {overall_accuracy/len(train_my_dataset)}')\n",
        "\n"
      ],
      "metadata": {
        "id": "EtSYGS8UYD2I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9a89bda-5ae6-492d-db7e-1ae28b2b605f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 --> Accuracy 0.12242424242424242\n",
            "Epoch: 1 --> Accuracy 0.12242424242424242\n",
            "Epoch: 2 --> Accuracy 0.12242424242424242\n",
            "Epoch: 3 --> Accuracy 0.45406060606060605\n",
            "Epoch: 4 --> Accuracy 0.8775757575757576\n",
            "Epoch: 5 --> Accuracy 0.8775757575757576\n",
            "Epoch: 6 --> Accuracy 0.8775757575757576\n",
            "Epoch: 7 --> Accuracy 0.8775757575757576\n",
            "Epoch: 8 --> Accuracy 0.8775757575757576\n",
            "Epoch: 9 --> Accuracy 0.8775757575757576\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0HMMLnJ6iz5Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This week::\n",
        "#1. root word for bangla dictionary(Matters)\n",
        "#2. Bangla root word word2vec(pretrained)\n",
        "#3. Suffix add based on rules->bangal->banglai jai->jaccilam khai->khaccilam (bangla stemmer papers)\n"
      ],
      "metadata": {
        "id": "wk_xTGRyuwol"
      }
    }
  ]
}