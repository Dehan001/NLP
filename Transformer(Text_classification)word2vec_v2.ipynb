{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "***Task***\n",
        "\n",
        "Reddit comments: Data\n",
        "\n",
        "10 words->sentence length\n",
        "\n",
        "10 words+eos\n",
        "\n",
        "preprocessing\n",
        "\n",
        "one hot encoding and word2vec\n",
        "\n",
        "Transformer(single head)\n",
        "\n",
        "class"
      ],
      "metadata": {
        "id": "S0sm93qPes7e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import tensorflow.keras\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import InputLayer\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import MultiHeadAttention, LayerNormalization, Dropout, Layer\n",
        "from tensorflow.keras.layers import Embedding, Input, GlobalAveragePooling1D, Dense\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import gensim\n",
        "import gensim.downloader"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3SnzBphafVx_",
        "outputId": "fa12426a-540f-40b2-d928-2b309d0570aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def words(str):\n",
        "  tokens=str.lower()\n",
        "  tokens=str.split()\n",
        "  str1=''\n",
        "\n",
        "  for i in range(len(tokens)):\n",
        "    if i==0:\n",
        "      str1+=tokens[i]\n",
        "    else:\n",
        "\n",
        "      str1=str1+\" \"+tokens[i]\n",
        "  return str1.lower()"
      ],
      "metadata": {
        "id": "ezgA6HmdJLwr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cleaningPunc(str):\n",
        "  str=re.sub(r'[^\\w\\s]','',str)\n",
        "  return str"
      ],
      "metadata": {
        "id": "VIGaoSCeNOwk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def urlremove(sentence):\n",
        "  sentence=re.sub(r'http\\S+', '', sentence)\n",
        "  sentence=re.sub(r'www\\S+', '', sentence)\n",
        "  return sentence\n",
        ""
      ],
      "metadata": {
        "id": "iY9CYcHwNe1G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def stopwordsRemove(str):\n",
        "  str1=\"\"\n",
        "  stop_words=stopwords.words('english')\n",
        "  str=str.split()\n",
        "\n",
        "  for word in str:\n",
        "    if not word in stop_words:\n",
        "      str1+=word+' '\n",
        "  return str1\n"
      ],
      "metadata": {
        "id": "C0g8Lg2oNjS2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cleaningdigits(str):\n",
        "  str=re.sub(r'[\\d+]','',str)\n",
        "  return str"
      ],
      "metadata": {
        "id": "iiBCRIjiNqZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmatization(str):\n",
        "  lemma=WordNetLemmatizer()\n",
        "  str1=''\n",
        "  tokens=str.split()\n",
        "  for word in tokens:\n",
        "    store=lemma.lemmatize(word)\n",
        "    str1+=store+' '\n",
        "  return str1.lower()\n"
      ],
      "metadata": {
        "id": "z9GMVc7KN4B3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bOZAO80sb6UW",
        "outputId": "792fb3a5-800d-4ef9-9252-29e8be83ef21"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3609,)\n"
          ]
        }
      ],
      "source": [
        "url='https://drive.google.com/file/d/1xEyhcHjrjEo62k84kKu6mI9vttGtc-jV/view?usp=share_link'\n",
        "url='https://drive.google.com/uc?id=' + url.split('/')[-2]\n",
        "df = pd.read_csv(url)\n",
        "df=df.drop_duplicates()\n",
        "df=df[df['Message'].notnull()]\n",
        "df=df[df['Category'].notnull()]\n",
        "\n",
        "label_encoder=preprocessing.LabelEncoder()\n",
        "df['Category']=label_encoder.fit_transform(df['Category'])\n",
        "\n",
        "texts=df['Message'].to_numpy()\n",
        "labels=df['Category'].to_numpy()\n",
        "\n",
        "x_train,x_test,y_train,y_test=train_test_split(texts,labels,test_size=0.3)\n",
        "\n",
        "\n",
        "print(x_train.shape)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(x_train)):\n",
        "  sentence=x_train[i]\n",
        "\n",
        "  sentence=words(sentence)\n",
        "\n",
        "  sentence=cleaningPunc(sentence)\n",
        "\n",
        "  sentence=stopwordsRemove(sentence)\n",
        "\n",
        "  sentence=cleaningdigits(sentence)\n",
        "\n",
        "  sentence=lemmatization(sentence)\n",
        "\n",
        "  sentence=urlremove(sentence)\n",
        "  x_train[i]=sentence\n"
      ],
      "metadata": {
        "id": "-HoKHbxtOMm1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(x_test)):\n",
        "  sentence=x_test[i]\n",
        "\n",
        "  sentence=words(sentence)\n",
        "\n",
        "  sentence=cleaningPunc(sentence)\n",
        "\n",
        "  sentence=stopwordsRemove(sentence)\n",
        "\n",
        "  sentence=cleaningdigits(sentence)\n",
        "\n",
        "  sentence=lemmatization(sentence)\n",
        "\n",
        "  sentence=urlremove(sentence)\n",
        "  x_test[i]=sentence"
      ],
      "metadata": {
        "id": "GLV8jR50ON5h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#new_model= gensim.downloader.load('fasttext-wiki-news-subwords-300')\n",
        "new_model=gensim.downloader.load('glove-twitter-25')\n",
        "new_model.most_similar(\"bad\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qlk8CGyJqy-3",
        "outputId": "ca8a4414-f8dd-43d5-c529-ac1415e9b6f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[===============================================---] 95.4% 100.0/104.8MB downloaded\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('shit', 0.9544215202331543),\n",
              " ('crazy', 0.9532092213630676),\n",
              " ('but', 0.952229380607605),\n",
              " ('hell', 0.9521805047988892),\n",
              " ('right', 0.9486410617828369),\n",
              " ('like', 0.948320209980011),\n",
              " ('same', 0.9475184082984924),\n",
              " ('damn', 0.94697105884552),\n",
              " ('thing', 0.9445644617080688),\n",
              " ('way', 0.9423031210899353)]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word='eating'\n",
        "vec=new_model[word]\n",
        "print(len(vec))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SosoEEa95yhQ",
        "outputId": "9941e420-6b57-4b69-8b4c-112081fe41ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token_index={}\n",
        "counter=0\n",
        "for sentence in x_train:\n",
        "  tokens=sentence.split()\n",
        "  for token in tokens:\n",
        "    if not token in token_index:\n",
        "      token_index.update({token:counter+1})\n",
        "      counter=counter+1\n",
        "print(len(token_index))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9d0WJ2q5OS2p",
        "outputId": "714c14d2-9000-4171-d4c9-6ac7ee9ce005"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6431\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "maxlen=len(token_index)\n",
        "wv_size=len(vec)\n",
        "results  = np.zeros(shape = (len(x_train),\n",
        "                            maxlen,\n",
        "                            wv_size))\n",
        "print(results.shape)\n",
        "\n",
        "results_test  = np.zeros(shape = (len(x_test),\n",
        "                            maxlen,\n",
        "                            wv_size))\n",
        "print(results_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k5JekUgWOabc",
        "outputId": "f0caf8a6-1f1e-4139-a118-145fe2ed9350"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3609, 6431, 25)\n",
            "(1548, 6431, 25)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i, documents in enumerate(x_train):\n",
        "  for j, considered_word in list(enumerate(documents.split())):\n",
        "    try:\n",
        "      results[i, j, :] = new_model[considered_word]\n",
        "    except:\n",
        "      results[i,j,:]=np.zeros((wv_size))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yzVqP-g7O8qs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, documents in enumerate(x_test):\n",
        "  for j, considered_word in list(enumerate(documents.split())):\n",
        "    try:\n",
        "      results[i, j, :] = new_model[considered_word]\n",
        "    except:\n",
        "      results[i,j,:]=np.zeros((wv_size))\n"
      ],
      "metadata": {
        "id": "K60SNFkmO4xD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(Layer):\n",
        "    def __init__(self, embed_dim, ff_dim, num_heads=1, rate=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.att = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.ffn = Sequential(\n",
        "            [Dense(ff_dim, activation=\"relu\"),\n",
        "             Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = Dropout(rate)\n",
        "        self.dropout2 = Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        attn_output = self.att(inputs, inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)"
      ],
      "metadata": {
        "id": "WZ0EsltePNLu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TokenAndPositionEmbedding(Layer):\n",
        "    def __init__(self):\n",
        "        super(TokenAndPositionEmbedding, self).__init__()\n",
        "\n",
        "\n",
        "    def call(self, x):\n",
        "      document_len=len(x)\n",
        "      n=10000\n",
        "      #positions=np.zeros((document_len,maxlen,vocab_size))\n",
        "      positions=np.zeros((maxlen,vocab_size))\n",
        "      #for i in range(document_len):\n",
        "      for j in range(maxlen):\n",
        "        for k in np.arange(int(vocab_size/2)):\n",
        "          denominator=np.power(n,2*k/vocab_size)\n",
        "          positions[j,2*k]=np.sin(j/denominator)\n",
        "          positions[j,2*k+1]=np.cos(j/denominator)\n",
        "\n",
        "      return x + positions"
      ],
      "metadata": {
        "id": "BLOxHqastv9F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size=wv_size\n",
        "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
        "\n",
        "inputs = Input(shape=(maxlen,vocab_size))\n",
        "\n",
        "embedding_layer = TokenAndPositionEmbedding()\n",
        "x = embedding_layer(inputs)\n",
        "print(x.shape)\n",
        "transformer_block = TransformerBlock(vocab_size, ff_dim)\n",
        "x=transformer_block(x)\n",
        "x=transformer_block(x)\n",
        "x=transformer_block(x)\n",
        "x=transformer_block(x)\n",
        "x=transformer_block(x)\n",
        "x=transformer_block(x)\n",
        "#x=tf.keras.backend.mean(\n",
        "#    x,\n",
        "#    axis=1,\n",
        "#    keepdims=False\n",
        "#)\n",
        "x = GlobalAveragePooling1D()(x)\n",
        "x = Dropout(0.1)(x)\n",
        "x = Dense(20, activation=\"relu\")(x)\n",
        "x = Dropout(0.1)(x)\n",
        "outputs = Dense(6, activation=\"softmax\")(x)\n",
        "print(outputs.shape)\n",
        "model = Model(inputs=inputs, outputs=outputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2mGsR01XtwKm",
        "outputId": "0cc35be7-ba78-4ca9-8bf1-7075988a3b25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(None, 6431, 25)\n",
            "(None, 6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "history = model.fit(results, y_train,\n",
        "                    batch_size=128, epochs=32,\n",
        "                    validation_data=(results_test, y_train)\n",
        "                   )"
      ],
      "metadata": {
        "id": "8jAieiTWQk26",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6bf77c2-9f5c-4a83-93dd-ec4743098f52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results_calc = model.evaluate(results_test, y_test, verbose=2)\n",
        "\n",
        "for name, value in zip(model.metrics_names, results_calc):\n",
        "    print(\"%s: %.3f\" % (name, value))"
      ],
      "metadata": {
        "id": "JmVTGZQkRLS-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3GDykg3iMDVM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}