{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ov26AIR19Plv",
        "outputId": "55a65426-9210-4566-99cd-009793c1e1a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.29.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.10/dist-packages (0.0.53)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacremoses) (2022.10.31)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from sacremoses) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses) (1.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sacremoses) (4.65.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install sacremoses\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3SnzBphafVx_",
        "outputId": "a701e35f-d25b-458a-8519-2cd1d1294256"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader,Dataset\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import gensim\n",
        "import gensim.downloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MTmWilHbUVAQ"
      },
      "outputs": [],
      "source": [
        "class Textpreprocessing:\n",
        "  def __init__(self,text):\n",
        "    self.text=text\n",
        "\n",
        "  def words(self,str1):\n",
        "    tokens=str1.split()[:4]\n",
        "    str2=''\n",
        "\n",
        "    for i in range(len(tokens)):\n",
        "      if i==0:\n",
        "        str2+=tokens[i]\n",
        "      else:\n",
        "        str2=str2+\" \"+tokens[i]\n",
        "\n",
        "    return str2\n",
        "\n",
        "\n",
        "\n",
        "  def stopword_remove(self,str1):\n",
        "    str1=str1.lower()\n",
        "    tokens=str1.split()\n",
        "    str2=\"\"\n",
        "    stop_words=stopwords.words('english')\n",
        "    for word in tokens:\n",
        "      if not word in stop_words:\n",
        "        str2+=word+' '\n",
        "    return str2\n",
        "\n",
        "  def url_remove(self,str1):\n",
        "    str1=re.sub(r'http\\S+', '', str1)\n",
        "    str1=re.sub(r'www\\S+', '', str1)\n",
        "    return str1\n",
        "\n",
        "  def clean_punctuation(self,str1):\n",
        "    str1=re.sub(r'[^\\w\\s]','',str1)\n",
        "    return str1\n",
        "\n",
        "  def cleaningdigits(self,str1):\n",
        "    str1=re.sub(r'[\\d+]','',str1)\n",
        "    return str1\n",
        "\n",
        "  def lemmatization(self,str1):\n",
        "    lemma=WordNetLemmatizer()\n",
        "    str2=''\n",
        "    tokens=str1.split()\n",
        "    for word in tokens:\n",
        "      store=lemma.lemmatize(word)\n",
        "      str2+=store+' '\n",
        "    return str2\n",
        "\n",
        "  def preprocess(self):\n",
        "    self.text=self.text.apply(self.words)\n",
        "    self.text=self.text.apply(self.stopword_remove)\n",
        "    self.text=self.text.apply(self.url_remove)\n",
        "    self.text=self.text.apply(self.clean_punctuation)\n",
        "    self.text=self.text.apply(self.cleaningdigits)\n",
        "    self.text=self.text.apply(self.lemmatization)\n",
        "\n",
        "    return self.text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_q4uzRetArUX"
      },
      "outputs": [],
      "source": [
        "def convert_to_number(label):\n",
        "  if label=='spam':\n",
        "    return 1\n",
        "  return 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a58dwFRBixDF",
        "outputId": "74e2c99b-a02e-422e-c335-7605ccd6558e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<bound method NDFrame.head of      Category                                            Message\n",
            "0         ham  Go until jurong point, crazy.. Available only ...\n",
            "1         ham                      Ok lar... Joking wif u oni...\n",
            "2        spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
            "3         ham  U dun say so early hor... U c already then say...\n",
            "4         ham  Nah I don't think he goes to usf, he lives aro...\n",
            "...       ...                                                ...\n",
            "5567     spam  This is the 2nd time we have tried 2 contact u...\n",
            "5568      ham               Will Ã¼ b going to esplanade fr home?\n",
            "5569      ham  Pity, * was in mood for that. So...any other s...\n",
            "5570      ham  The guy did some bitching but I acted like i'd...\n",
            "5571      ham                         Rofl. Its true to its name\n",
            "\n",
            "[5157 rows x 2 columns]>\n"
          ]
        }
      ],
      "source": [
        "url='https://drive.google.com/file/d/1xEyhcHjrjEo62k84kKu6mI9vttGtc-jV/view?usp=share_link'\n",
        "url='https://drive.google.com/uc?id=' + url.split('/')[-2]\n",
        "df = pd.read_csv(url)\n",
        "df=df.drop_duplicates()\n",
        "df=df[df['Message'].notnull()]\n",
        "df=df[df['Category'].notnull()]\n",
        "\n",
        "print(df.head)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KeyjyziLZoLz"
      },
      "outputs": [],
      "source": [
        "text_preprocess=Textpreprocessing(df['Message'])\n",
        "df['Message']=text_preprocess.preprocess()\n",
        "document_size=len(df)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s2dlmepcUpLi"
      },
      "outputs": [],
      "source": [
        "df['Numeric_label']=df['Category'].apply(convert_to_number)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w3EacCy4TGuW"
      },
      "outputs": [],
      "source": [
        "#change glove overall\n",
        "new_model=gensim.downloader.load('glove-twitter-25')\n",
        "new_model.most_similar(\"bad\")\n",
        "wv_size=len(new_model['bad'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5pPsHts4ZzzA"
      },
      "outputs": [],
      "source": [
        "def create_word2vec_for_setence(text,max_word):\n",
        "\n",
        "  results  = np.zeros(shape = (max_word,wv_size))\n",
        "  #for i, documents in enumerate(text):\n",
        "  for j, considered_word in list(enumerate(text.split())):\n",
        "    try:\n",
        "      results[j, :] = new_model[considered_word]\n",
        "    except:\n",
        "      results[j,:]=np.zeros((wv_size))\n",
        "  return torch.tensor(results,dtype=torch.long)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M-wgPnVYjfca"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from transformers import TransfoXLLMHeadModel\n",
        "def create_xl_output(tensor):\n",
        "  model_name = 'transfo-xl-wt103'\n",
        "  model = TransfoXLLMHeadModel.from_pretrained(model_name)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    #hidden_states = model.get_input_embeddings()(tensor)\n",
        "    outputs = model(tensor)\n",
        "  logits = outputs.logits\n",
        "  logits=logits[:,-1,:]\n",
        "  logits=torch.tensor(logits,dtype=torch.float)\n",
        "  #sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True\n",
        "  return logits.clone().detach()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wrH2qggIUxtK"
      },
      "outputs": [],
      "source": [
        "class MyDataset(Dataset):\n",
        "  def __init__(self,df):\n",
        "    super().__init__()\n",
        "    self.texts=df['Message'].values\n",
        "    self.labels=df['Numeric_label'].values\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.texts)\n",
        "\n",
        "  def __getitem__(self,idx):\n",
        "    text=self.texts[idx]\n",
        "    label=self.labels[idx]\n",
        "\n",
        "    word2vec=create_word2vec_for_setence(text,4)\n",
        "    tensor=create_xl_output(word2vec)\n",
        "    label=torch.tensor(label,dtype=torch.float32)\n",
        "\n",
        "    return tensor,label\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ck1z1JQaWXvO"
      },
      "outputs": [],
      "source": [
        "train_dataset,test_dataset=train_test_split(df, test_size=0.2, random_state=42)\n",
        "train_dataset=train_dataset[:10]\n",
        "test_dataset=test_dataset[:10]\n",
        "train_my_dataset=MyDataset(train_dataset)\n",
        "test_my_dataset=MyDataset(test_dataset)\n",
        "train_dataloader=DataLoader(train_my_dataset,batch_size=32,shuffle=True)\n",
        "test_dataloader=DataLoader(test_my_dataset,batch_size=32,shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wURpuFK6VJxZ"
      },
      "outputs": [],
      "source": [
        "#update input_dim and hidden_dim\n",
        "#why requires_grad=False\n",
        "class MyModel(nn.Module):\n",
        "  def __init__(self,input_dim=267735,hidden_dim=64):\n",
        "    super().__init__()\n",
        "    self.input_dim=input_dim\n",
        "    self.hidden_dim=hidden_dim\n",
        "    self.linear_layer1=nn.Linear(self.input_dim,self.hidden_dim)\n",
        "    self.linear_layer2=nn.Linear(self.hidden_dim,self.hidden_dim)\n",
        "    self.final_layer=nn.Linear(self.hidden_dim,1)\n",
        "    self.sigmoid=nn.Sigmoid()\n",
        "\n",
        "  def forward(self,x):\n",
        "\n",
        "    x=torch.mean(x,1)\n",
        "    x.requires_grad=True\n",
        "\n",
        "    #x=x.view(batch_size,-1)\n",
        "    #x=torch.conacte()\n",
        "    x=self.linear_layer1(x)\n",
        "    x=self.sigmoid(x)\n",
        "    x=self.linear_layer2(x)\n",
        "    x=self.sigmoid(x)\n",
        "    x=self.final_layer(x)\n",
        "    #print('requires_grad',x.requires_grad)\n",
        "    x=self.sigmoid(x)\n",
        "\n",
        "\n",
        "    return x\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gfRXxGliX_Wh"
      },
      "outputs": [],
      "source": [
        "model=MyModel()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "67HWsTYGYkcT"
      },
      "outputs": [],
      "source": [
        "criterion=nn.BCELoss(reduction='mean')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kYuSSlZ0YpGX"
      },
      "outputs": [],
      "source": [
        "optimizer=torch.optim.Adam(model.parameters(),lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ixfj6kPhynO5"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "EtSYGS8UYD2I",
        "outputId": "6d00648a-d6d4-4acd-c98c-e8f5b04bced8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-63-b78efe9cd3e4>:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  return torch.tensor(logits,dtype=torch.float)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 0 --> Accuracy 0.9&\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.90      1.00      0.95         9\n",
            "         1.0       0.00      0.00      0.00         1\n",
            "\n",
            "    accuracy                           0.90        10\n",
            "   macro avg       0.45      0.50      0.47        10\n",
            "weighted avg       0.81      0.90      0.85        10\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "<ipython-input-63-b78efe9cd3e4>:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  return torch.tensor(logits,dtype=torch.float)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1 --> Accuracy 0.9&\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.90      1.00      0.95         9\n",
            "         1.0       0.00      0.00      0.00         1\n",
            "\n",
            "    accuracy                           0.90        10\n",
            "   macro avg       0.45      0.50      0.47        10\n",
            "weighted avg       0.81      0.90      0.85        10\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "<ipython-input-63-b78efe9cd3e4>:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  return torch.tensor(logits,dtype=torch.float)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 2 --> Accuracy 0.9&\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.90      1.00      0.95         9\n",
            "         1.0       0.00      0.00      0.00         1\n",
            "\n",
            "    accuracy                           0.90        10\n",
            "   macro avg       0.45      0.50      0.47        10\n",
            "weighted avg       0.81      0.90      0.85        10\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "\n",
        "for epoch in range(3):\n",
        "  overall_accuracy=0\n",
        "  for x,y in train_dataloader:\n",
        "    predicted_y=model(x)\n",
        "\n",
        "    batch_size=x.shape[0]\n",
        "    y=y.view(batch_size,1)\n",
        "\n",
        "    loss=criterion(predicted_y,y)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "    y_true=y.detach().numpy()\n",
        "\n",
        "    y_pred=predicted_y.detach().numpy() >0.5\n",
        "\n",
        "\n",
        "\n",
        "    accuracy= accuracy_score(y_true,y_pred)\n",
        "\n",
        "    overall_accuracy +=accuracy*batch_size\n",
        "\n",
        "\n",
        "  print(f'Epoch: {epoch} --> Accuracy {overall_accuracy/len(train_my_dataset)}&')\n",
        "\n",
        "  print(classification_report(y_true, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-F2YzE6a5Mb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cde_5MljJ6ps"
      },
      "source": [
        "##Extra"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0HMMLnJ6iz5Q"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "def shape_list(x):\n",
        "    \"\"\"Deal with dynamic shape in tensorflow cleanly.\"\"\"\n",
        "    static = x.shape.as_list()\n",
        "    dynamic = tf.shape(x)\n",
        "    return [dynamic[i] if s is None else s for i, s in enumerate(static)]\n",
        "\n",
        "\n",
        "class PositionalEmbedding(tf.keras.layers.Layer):\n",
        "    def __init__(self, demb, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        self.inv_freq = 1 / (10000 ** (tf.range(0, demb, 2.0) / demb))\n",
        "\n",
        "    def call(self, pos_seq, bsz=None):\n",
        "        sinusoid_inp = tf.einsum(\"i,j->ij\", pos_seq, self.inv_freq)\n",
        "        pos_emb = tf.concat([tf.sin(sinusoid_inp), tf.cos(sinusoid_inp)], -1)\n",
        "\n",
        "        if bsz is not None:\n",
        "            return tf.tile(pos_emb[:, None, :], [1, bsz, 1])\n",
        "        else:\n",
        "            return pos_emb[:, None, :]\n",
        "\n",
        "\n",
        "class PositionwiseFF(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, d_inner, dropout, kernel_initializer,\n",
        "                 pre_lnorm=False, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.d_inner = d_inner\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.layer_1 = tf.keras.layers.Dense(\n",
        "            d_inner, kernel_initializer=kernel_initializer, activation=tf.nn.relu, name='layer_1'\n",
        "        )\n",
        "        self.drop_1 = tf.keras.layers.Dropout(dropout, name='drop_1')\n",
        "        self.layer_2 = tf.keras.layers.Dense(d_model, kernel_initializer=kernel_initializer, name='layer_2')\n",
        "        self.drop_2 = tf.keras.layers.Dropout(dropout, name='drop_2')\n",
        "\n",
        "        self.layer_norm = tf.keras.layers.LayerNormalization(epsilon=1e-6, name='layer_norm')\n",
        "        self.pre_lnorm = pre_lnorm\n",
        "\n",
        "    def call(self, inp, training=False):\n",
        "        if self.pre_lnorm:\n",
        "            # layer normalization + positionwise feed-forward\n",
        "            core_out = self.layer_norm(inp)\n",
        "            core_out = self.layer_1(core_out)\n",
        "            core_out = self.drop_1(core_out, training=training)\n",
        "            core_out = self.layer_2(core_out)\n",
        "            core_out = self.drop_2(core_out, training=training)\n",
        "\n",
        "            output = [core_out + inp]\n",
        "        else:\n",
        "            # positionwise feed-forward\n",
        "            core_out = self.layer_1(inp)\n",
        "            core_out = self.drop_1(core_out, training=training)\n",
        "            core_out = self.layer_2(core_out)\n",
        "            core_out = self.drop_2(core_out, training=training)\n",
        "\n",
        "            output = [self.layer_norm(inp + core_out)]\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "class RelativeMultiHeadAttn(tf.keras.layers.Layer):\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_head,\n",
        "        d_model,\n",
        "        d_head,\n",
        "        dropout,\n",
        "        dropatt,\n",
        "        kernel_initializer,\n",
        "        pre_lnorm=False,\n",
        "        r_r_bias=None,\n",
        "        r_w_bias=None,\n",
        "        **kwargs\n",
        "    ):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        self.n_head = n_head\n",
        "        self.d_model = d_model\n",
        "        self.d_head = d_head\n",
        "        self.dropout = dropout\n",
        "        self.kernel_initializer=kernel_initializer\n",
        "\n",
        "        self.qkv_net = tf.keras.layers.Dense(\n",
        "            3 * n_head * d_head, kernel_initializer=kernel_initializer, use_bias=False, name=\"qkv\"\n",
        "        )\n",
        "        self.r_net = tf.keras.layers.Dense(\n",
        "            self.n_head * self.d_head, kernel_initializer=kernel_initializer, use_bias=False, name=\"r\"\n",
        "        )\n",
        "        self.drop = tf.keras.layers.Dropout(dropout)\n",
        "        self.dropatt = tf.keras.layers.Dropout(dropatt)\n",
        "        self.o_net = tf.keras.layers.Dense(\n",
        "            d_model, kernel_initializer=kernel_initializer, use_bias=False, name=\"o\"\n",
        "        )\n",
        "        self.layer_norm = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.scale = 1 / (d_head ** 0.5)\n",
        "\n",
        "        if r_r_bias is not None and r_w_bias is not None:  # Biases are shared\n",
        "            self.r_r_bias = r_r_bias\n",
        "            self.r_w_bias = r_w_bias\n",
        "        else:\n",
        "            self.r_r_bias = self.add_weight(\n",
        "                shape=(self.n_head, self.d_head), initializer=\"zeros\", trainable=True, name=\"r_r_bias\"\n",
        "            )\n",
        "            self.r_w_bias = self.add_weight(\n",
        "                shape=(self.n_head, self.d_head), initializer=\"zeros\", trainable=True, name=\"r_w_bias\"\n",
        "            )\n",
        "\n",
        "        self.pre_lnorm = pre_lnorm\n",
        "\n",
        "    def _rel_shift(self, x):\n",
        "        x_size = shape_list(x)\n",
        "\n",
        "        x = tf.pad(x, [[0, 0], [1, 0], [0, 0], [0, 0]])\n",
        "        x = tf.reshape(x, [x_size[1] + 1, x_size[0], x_size[2], x_size[3]])\n",
        "        x = tf.slice(x, [1, 0, 0, 0], [-1, -1, -1, -1])\n",
        "        x = tf.reshape(x, x_size)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        w, r, attn_mask, mems = inputs\n",
        "        qlen, rlen, bsz = shape_list(w)[0], shape_list(r)[0], shape_list(w)[1]\n",
        "\n",
        "        if mems is not None:\n",
        "            cat = tf.concat([mems, w], 0)\n",
        "        else:\n",
        "            cat = w\n",
        "\n",
        "        if self.pre_lnorm:\n",
        "            cat = self.layer_norm(cat)\n",
        "\n",
        "        w_heads = self.qkv_net(cat)\n",
        "        r_head_k = self.r_net(r)\n",
        "\n",
        "        w_head_q, w_head_k, w_head_v = tf.split(w_heads, 3, axis=-1)\n",
        "        w_head_q = w_head_q[-qlen:]\n",
        "\n",
        "        klen = shape_list(w_head_k)[0]\n",
        "\n",
        "        w_head_q = tf.reshape(w_head_q, (qlen, bsz, self.n_head, self.d_head))\n",
        "        w_head_k = tf.reshape(w_head_k, (klen, bsz, self.n_head, self.d_head))\n",
        "        w_head_v = tf.reshape(w_head_v, (klen, bsz, self.n_head, self.d_head))\n",
        "\n",
        "        r_head_k = tf.reshape(r_head_k, (rlen, self.n_head, self.d_head))\n",
        "\n",
        "        rw_head_q = w_head_q + self.r_w_bias\n",
        "        rr_head_q = w_head_q + self.r_r_bias\n",
        "\n",
        "        AC = tf.einsum(\"ibnd,jbnd->ijbn\", rw_head_q, w_head_k)\n",
        "        BD = tf.einsum(\"ibnd,jnd->ijbn\", rr_head_q, r_head_k)\n",
        "        BD = self._rel_shift(BD)\n",
        "\n",
        "        attn_score = AC + BD\n",
        "        attn_score = attn_score * self.scale\n",
        "\n",
        "        attn_mask_t = attn_mask[:, :, None, None]\n",
        "        attn_score = attn_score * (1 - attn_mask_t) - 1e30 * attn_mask_t\n",
        "\n",
        "        attn_prob = tf.nn.softmax(attn_score, axis=1)\n",
        "        attn_prob = self.dropatt(attn_prob, training=training)\n",
        "\n",
        "        attn_vec = tf.einsum(\"ijbn,jbnd->ibnd\", attn_prob, w_head_v)\n",
        "        size_t = shape_list(attn_vec)\n",
        "        attn_vec = tf.reshape(attn_vec, (size_t[0], size_t[1], self.n_head * self.d_head))\n",
        "\n",
        "        attn_out = self.o_net(attn_vec)\n",
        "        attn_out = self.drop(attn_out, training=training)\n",
        "\n",
        "        if self.pre_lnorm:\n",
        "            outputs = [w + attn_out]\n",
        "        else:\n",
        "            outputs = [self.layer_norm(w + attn_out)]\n",
        "\n",
        "        return outputs\n",
        "\n",
        "\n",
        "class TransformerXLLayer(tf.keras.layers.Layer):\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_head,\n",
        "        d_model,\n",
        "        d_head,\n",
        "        d_inner,\n",
        "        dropout,\n",
        "        dropatt,\n",
        "        initializer,\n",
        "        pre_lnorm=False,\n",
        "        r_w_bias=None,\n",
        "        r_r_bias=None,\n",
        "        **kwargs\n",
        "    ):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        self.n_head = n_head\n",
        "        self.d_model = d_model\n",
        "        self.d_head = d_head\n",
        "        self.d_inner = d_inner\n",
        "        self.dropout = dropout\n",
        "        self.dropatt = dropatt\n",
        "        self.initializer = initializer\n",
        "        self.pre_lnorm = pre_lnorm\n",
        "\n",
        "        self.xltran_attn = RelativeMultiHeadAttn(\n",
        "            n_head=self.n_head,\n",
        "            d_model=self.d_model,\n",
        "            d_head=self.d_head,\n",
        "            dropout=self.dropout,\n",
        "            dropatt=self.dropatt,\n",
        "            kernel_initializer=self.initializer,\n",
        "            pre_lnorm=self.pre_lnorm,\n",
        "            r_w_bias=r_w_bias,\n",
        "            r_r_bias=r_r_bias,\n",
        "            name=\"xltran_attn\",\n",
        "        )\n",
        "        self.pos_ff = PositionwiseFF(\n",
        "            d_model=self.d_model,\n",
        "            d_inner=self.d_inner,\n",
        "            dropout=self.dropout,\n",
        "            kernel_initializer=self.initializer,\n",
        "            pre_lnorm=self.pre_lnorm,\n",
        "            name=\"pos_ff\",\n",
        "        )\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        inp, r, attn_mask, mems = inputs\n",
        "        attn_outputs = self.xltran_attn([inp, r, attn_mask, mems], training=training)\n",
        "        ff_output = self.pos_ff(attn_outputs[0], training=training)\n",
        "\n",
        "        outputs = [ff_output[0]]\n",
        "\n",
        "        return outputs\n",
        "\n",
        "\n",
        "class AdaptiveEmbedding(tf.keras.layers.Layer):\n",
        "    def __init__(self, n_token, d_embed, d_proj, cutoffs, initializer, \\\n",
        "                 proj_initializer=None, div_val=1, proj_same_dim=True, \\\n",
        "                 use_tpu=True, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        self.n_token = n_token\n",
        "        self.d_embed = d_embed\n",
        "        self.d_proj = d_proj\n",
        "\n",
        "        self.cutoffs = cutoffs + [n_token]\n",
        "        self.cutoff_ends = [0] + self.cutoffs\n",
        "\n",
        "        self.initializer = initializer\n",
        "        self.proj_initializer = proj_initializer if proj_initializer is not None else initializer\n",
        "\n",
        "        self.div_val = div_val\n",
        "        self.proj_same_dim = proj_same_dim\n",
        "\n",
        "        self.use_tpu = use_tpu\n",
        "\n",
        "        self.emb_scale = d_proj ** 0.5\n",
        "\n",
        "        self.emb_weights = []\n",
        "        self.emb_projs = []\n",
        "\n",
        "        for i in range(len(self.cutoffs)):\n",
        "            l_idx, r_idx = self.cutoff_ends[i], self.cutoff_ends[i + 1]\n",
        "            d_emb_i = self.d_embed // (self.div_val ** i)\n",
        "            self.emb_weights.append(\n",
        "                self.add_weight(\n",
        "                    shape=(r_idx - l_idx, d_emb_i),\n",
        "                    initializer=self.initializer,\n",
        "                    name=\"emb_weights_._{}\".format(i),\n",
        "                )\n",
        "            )\n",
        "            if d_emb_i == d_proj and \\\n",
        "                    (not self.proj_same_dim or self.div_val == 1):\n",
        "                self.emb_projs.append(None)\n",
        "            else:\n",
        "                self.emb_projs.append(\n",
        "                    self.add_weight(\n",
        "                        shape=(d_emb_i, self.d_proj),\n",
        "                        initializer=self.proj_initializer,\n",
        "                        trainable=True,\n",
        "                        name=\"emb_projs_._{}\".format(i),\n",
        "                    )\n",
        "                )\n",
        "\n",
        "    def get_weights(self):\n",
        "        weights = {\"emb_layers\": [], \"emb_projs\": []}\n",
        "        for i in range(len(self.emb_layers)):\n",
        "            weights[\"emb_layers\"].append(self.emb_layers[i].get_weights())\n",
        "            weights[\"emb_projs\"].append(self.emb_projs[i])\n",
        "        return weights\n",
        "\n",
        "    @staticmethod\n",
        "    def _embedding_lookup(lookup_table, x, use_tpu=False):\n",
        "        if use_tpu:\n",
        "            n_token = shape_list(lookup_table)[0]\n",
        "            one_hot_idx = tf.one_hot(x, n_token)\n",
        "            if one_hot_idx.shape.ndims == 2:\n",
        "                return tf.einsum('nd,in->id', lookup_table, one_hot_idx)\n",
        "            else:\n",
        "                return tf.einsum('nd,ibn->ibd', lookup_table, one_hot_idx)\n",
        "        else:\n",
        "            return tf.nn.embedding_lookup(lookup_table, x)\n",
        "\n",
        "    def call(self, inp):\n",
        "        inp_flat = tf.reshape(inp, (-1,))\n",
        "        emb_flat = tf.zeros([shape_list(inp_flat)[0], self.d_proj])\n",
        "        for i in range(len(self.cutoffs)):\n",
        "            l_idx, r_idx = self.cutoff_ends[i], self.cutoff_ends[i + 1]\n",
        "\n",
        "            mask_i = (inp_flat >= l_idx) & (inp_flat < r_idx)\n",
        "            inp_i = tf.minimum(inp_flat, r_idx-1)\n",
        "            inp_i = tf.maximum(inp_i-l_idx, 0)\n",
        "            emb_i = self._embedding_lookup(self.emb_weights[i], inp_i, self.use_tpu)\n",
        "            if self.emb_projs[i] is not None:\n",
        "                emb_i = tf.einsum(\"id,de->ie\", emb_i, self.emb_projs[i])\n",
        "\n",
        "            mask_i = tf.tile(tf.reshape(mask_i, [-1, 1]), [1, self.d_proj])\n",
        "            emb_flat = tf.where(mask_i, emb_i, emb_flat)\n",
        "\n",
        "        embed_shape = shape_list(inp) + [self.d_proj]\n",
        "        embed = tf.reshape(emb_flat, embed_shape)\n",
        "\n",
        "        embed *= self.emb_scale\n",
        "\n",
        "        return embed\n",
        "\n",
        "\n",
        "class AdaptiveSoftmax(tf.keras.layers.Layer):\n",
        "    def __init__(self, n_token, d_embed, d_proj, cutoffs, tie_projs, \\\n",
        "                 initializer=None, proj_initializer=None, div_val=1, \\\n",
        "                 proj_same_dim=True, tied_to=None, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        self.n_token = n_token\n",
        "        self.d_embed = d_embed\n",
        "        self.d_proj = d_proj\n",
        "\n",
        "        self.cutoffs = cutoffs + [n_token]\n",
        "        self.cutoff_ends = [0] + self.cutoffs\n",
        "        self.n_clusters = len(self.cutoffs) - 1\n",
        "\n",
        "        self.div_val = div_val\n",
        "        self.proj_same_dim=True\n",
        "\n",
        "        self.tied_to = tied_to\n",
        "        assert tied_to is not None\n",
        "        self.tie_projs = tie_projs\n",
        "\n",
        "        self.out_weights = []\n",
        "        self.out_biases = []\n",
        "        self.out_projs = []\n",
        "\n",
        "        if self.n_clusters > 0:\n",
        "            self.cluster_weight = self.add_weight(\n",
        "                shape=(self.n_clusters, self.d_embed), initializer=\"zeros\", \\\n",
        "                    trainable=True, name=\"cluster_weight\"\n",
        "            )\n",
        "            self.cluster_bias = self.add_weight(\n",
        "                shape=(self.n_clusters,), initializer=\"zeros\", trainable=True, \\\n",
        "                    name=\"cluster_bias\"\n",
        "            )\n",
        "\n",
        "        for i, emb_weight in enumerate(self.tied_to.emb_weights):\n",
        "            self.out_weights.append(emb_weight)\n",
        "            vocab_size = shape_list(emb_weight)[0]\n",
        "            self.out_biases.append(\n",
        "                self.add_weight(\n",
        "                    shape=(vocab_size,),\n",
        "                    initializer=\"zeros\",\n",
        "                    trainable=True,\n",
        "                    name=\"out_layers_._{}_.bias\".\n",
        "                        format(i)\n",
        "                )\n",
        "            )\n",
        "\n",
        "        for i, emb_proj in enumerate(self.tied_to.emb_projs):\n",
        "            out_proj = emb_proj\n",
        "            if emb_proj is not None and not self.tie_projs[i]:\n",
        "                out_proj = self.add_weight(\n",
        "                    shape=shape_list(emb_proj),\n",
        "                    initializer=proj_initializer,\n",
        "                    trainable=True,\n",
        "                    name=\"out_projs_._{}\".format(i)\n",
        "                )\n",
        "            self.out_projs.append(out_proj)\n",
        "\n",
        "    @staticmethod\n",
        "    def _logit(x, W, b, proj=None):\n",
        "        y = x\n",
        "        if x.shape.ndims == 3:\n",
        "            if proj is not None:\n",
        "                y = tf.einsum(\"ibd,ed->ibe\", y, proj)\n",
        "            return tf.einsum(\"ibd,nd->ibn\", y, W) + b\n",
        "        else:\n",
        "            if proj is not None:\n",
        "                y = tf.einsum('id,ed->ie', y, proj)\n",
        "            return tf.einsum('id,nd->in', y, W) + b\n",
        "\n",
        "    @staticmethod\n",
        "    def _gather_logprob(logprob, target):\n",
        "        lp_size = shape_list(target)\n",
        "        r = tf.range(lp_size[0])\n",
        "        c = tf.range(lp_size[1])\n",
        "        C, R = tf.meshgrid(c, r)\n",
        "        idx = tf.stack([R, C, target], axis=2)\n",
        "        return tf.gather_nd(logprob, idx)\n",
        "\n",
        "    def call(self, inputs, return_mean=True):\n",
        "        hidden, target = inputs\n",
        "        head_logprob = 0\n",
        "        if self.n_clusters == 0:\n",
        "            output = self._logit(hidden, self.out_weights[0], self.out_biases[0], self.out_projs[0])\n",
        "            loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=target, logits=output)\n",
        "        else:\n",
        "            hidden_sizes = shape_list(hidden)\n",
        "            out = []\n",
        "            loss = tf.zeros(hidden_sizes[:2], dtype=tf.float32)\n",
        "            for i in range(len(self.cutoffs)):\n",
        "                l_idx, r_idx = self.cutoff_ends[i], self.cutoff_ends[i + 1]\n",
        "\n",
        "                mask = (target >= l_idx) & (target < r_idx)\n",
        "                cur_target = tf.minimum(target, r_idx-1)\n",
        "                cur_target = tf.maximum(cur_target-l_idx, 0)\n",
        "\n",
        "                cur_W = self.out_weights[i]\n",
        "                cur_b = self.out_biases[i]\n",
        "                cur_P = self.out_projs[i]\n",
        "\n",
        "                if i == 0:\n",
        "                    cur_W = tf.concat([cur_W, self.cluster_weight], 0)\n",
        "                    cur_b = tf.concat([cur_b, self.cluster_bias], 0)\n",
        "\n",
        "                    head_logit = self._logit(hidden, cur_W, cur_b, cur_P)\n",
        "                    head_logprob = tf.nn.log_softmax(head_logit)\n",
        "\n",
        "                    cur_loss = self._gather_logprob(head_logprob, cur_target)\n",
        "                    loss = tf.where(mask, cur_loss, loss)\n",
        "                else:\n",
        "                    tail_logit = self._logit(hidden, cur_W, cur_b, cur_P)\n",
        "                    tail_logprob = tf.nn.log_softmax(tail_logit)\n",
        "\n",
        "                    cluster_prob_idx = self.cutoffs[0] + i - 1\n",
        "                    logprob_i = head_logprob[..., cluster_prob_idx, None] + tail_logprob\n",
        "\n",
        "                    cur_loss = self._gather_logprob(logprob_i, cur_target)\n",
        "                    loss = tf.where(mask, cur_loss, loss)\n",
        "            loss = -loss\n",
        "        if return_mean:\n",
        "            loss = tf.reduce_mean(loss)\n",
        "\n",
        "        return loss\n",
        "\n",
        "\n",
        "class TransformerXL(tf.keras.Model):\n",
        "    def __init__(self, n_token, n_layer, d_model, d_embed, n_head, d_head, d_inner, \\\n",
        "                 dropout, dropatt, initializer, proj_initializer=None, pre_lnorm=False, tgt_len=None, \\\n",
        "                 mem_len=0, cutoffs=[], div_val=1, tie_projs=[], same_length=False, \\\n",
        "                 clamp_len=-1, untie_r=False, proj_same_dim=True, use_tpu=True):\n",
        "\n",
        "        super(TransformerXL, self).__init__()\n",
        "\n",
        "        self.n_token = n_token\n",
        "        self.n_layer = n_layer\n",
        "        self.d_model = d_model\n",
        "        self.d_embed = d_embed\n",
        "        self.n_head = n_head\n",
        "        self.d_head = d_head\n",
        "        self.d_inner = d_inner\n",
        "\n",
        "        self.tgt_len = tgt_len\n",
        "        self.mem_len = mem_len\n",
        "\n",
        "        self.dropout = dropout\n",
        "        self.dropatt = dropatt\n",
        "\n",
        "        self.cutoffs = cutoffs\n",
        "        self.div_val = div_val\n",
        "        self.tie_projs = tie_projs\n",
        "        self.same_length = same_length\n",
        "        self.clamp_len = clamp_len\n",
        "        self.untie_r = untie_r\n",
        "        self.proj_same_dim = proj_same_dim\n",
        "\n",
        "        self.initializer = initializer\n",
        "        self.proj_initializer = proj_initializer if proj_initializer is not None else initializer\n",
        "\n",
        "        self.pre_lnorm = pre_lnorm\n",
        "        self.use_tpu = use_tpu\n",
        "\n",
        "        self.embedding_layer = AdaptiveEmbedding(\n",
        "                n_token=self.n_token,\n",
        "                d_embed=self.d_embed,\n",
        "                d_proj=self.d_model,\n",
        "                cutoffs=self.cutoffs,\n",
        "                initializer=self.initializer,\n",
        "                proj_initializer=self.proj_initializer,\n",
        "                div_val=self.div_val,\n",
        "                proj_same_dim=self.proj_same_dim,\n",
        "                use_tpu=self.use_tpu,\n",
        "                name='emb_layer'\n",
        "            )\n",
        "        self.pos_emb = PositionalEmbedding(d_model)\n",
        "\n",
        "        self.emb_dropout = tf.keras.layers.Dropout(dropout, name='emb_drop')\n",
        "        self.pos_dropout = tf.keras.layers.Dropout(dropout, name='pos_drop')\n",
        "\n",
        "        if not self.untie_r:\n",
        "            self.r_w_bias = self.add_weight(\n",
        "                shape=(self.n_head, self.d_head), initializer=\"zeros\", trainable=True, name=\"r_w_bias\"\n",
        "            )\n",
        "            self.r_r_bias = self.add_weight(\n",
        "                shape=(self.n_head, self.d_head), initializer=\"zeros\", trainable=True, name=\"r_r_bias\"\n",
        "            )\n",
        "\n",
        "        self.tran_layers = []\n",
        "        for i in range(self.n_layer):\n",
        "            self.tran_layers.append(\n",
        "                TransformerXLLayer(\n",
        "                    n_head=self.n_head,\n",
        "                    d_model=self.d_model,\n",
        "                    d_head=self.d_head,\n",
        "                    d_inner=self.d_inner,\n",
        "                    dropout=self.dropout,\n",
        "                    dropatt=self.dropatt,\n",
        "                    initializer=self.initializer,\n",
        "                    pre_lnorm=self.pre_lnorm,\n",
        "                    r_w_bias=None if self.untie_r else self.r_w_bias,\n",
        "                    r_r_bias=None if self.untie_r else self.r_r_bias,\n",
        "                    name='layers_._{}'.format(i)\n",
        "                )\n",
        "            )\n",
        "\n",
        "        self.out_dropout = tf.keras.layers.Dropout(dropout, name='out_drop')\n",
        "        self.logsoftmax_layer = AdaptiveSoftmax(\n",
        "                n_token=self.n_token,\n",
        "                d_embed=self.d_embed,\n",
        "                d_proj=self.d_model,\n",
        "                cutoffs=self.cutoffs,\n",
        "                tie_projs=self.tie_projs,\n",
        "                initializer=self.initializer,\n",
        "                proj_initializer=self.proj_initializer,\n",
        "                div_val=self.div_val,\n",
        "                proj_same_dim=self.proj_same_dim,\n",
        "                tied_to=self.embedding_layer,\n",
        "                name='softmax_layer'\n",
        "            )\n",
        "\n",
        "    def reset_length(self, tgt_len, mem_len):\n",
        "        self.tgt_len = tgt_len\n",
        "        self.mem_len = mem_len\n",
        "\n",
        "    def init_mems(self, bsz, mem_len):\n",
        "        mems = []\n",
        "        for i in range(self.n_layer):\n",
        "            empty = tf.zeros([mem_len, bsz, self.d_model])\n",
        "            mems.append(empty)\n",
        "        return mems\n",
        "\n",
        "    def _update_mems(self, hids, mems):\n",
        "        if mems is None:\n",
        "            return None\n",
        "        assert len(hids) == len(mems), \"len(hids) != len(mems)\"\n",
        "        new_mems = []\n",
        "        for i in range(len(hids)):\n",
        "            cat = tf.concat([mems[i], hids[i]], axis=0)\n",
        "            cat = tf.stop_gradient(cat)\n",
        "            mlen = shape_list(mems[i])[0]\n",
        "            if mlen > 0:\n",
        "                new_mems.append(cat[-mlen:])\n",
        "            else:\n",
        "                shape = [mlen]+shape_list(cat)[1:]\n",
        "                new_mems.append(tf.zeros(shape))\n",
        "        return new_mems\n",
        "\n",
        "    def _create_mask(self, qlen, mlen, same_length=False):\n",
        "        attn_mask = tf.ones([qlen, qlen])\n",
        "        mask_u = tf.linalg.band_part(attn_mask, 0, -1)\n",
        "        mask_dia = tf.linalg.band_part(attn_mask, 0, 0)\n",
        "        attn_mask_pad = tf.zeros([qlen, mlen])\n",
        "        ret = tf.concat([attn_mask_pad, mask_u - mask_dia], 1)\n",
        "        if same_length:\n",
        "            mask_l = tf.linalg.band_part(attn_mask, -1, 0)\n",
        "            ret = tf.concat([ret[:, :qlen] + mask_l - mask_dia, ret[:, qlen:]], 1)\n",
        "        return ret\n",
        "\n",
        "    def call(self, inp, tgt, mems=None, return_mean=False, training=False):\n",
        "        # the original code for Transformer-XL used shapes [len, bsz]\n",
        "        # so we transpose here from shape [bsz, len] to shape [len, bsz]\n",
        "        inp = tf.transpose(inp, perm=(1, 0))\n",
        "        tgt = tf.transpose(tgt, perm=(1, 0))\n",
        "\n",
        "        qlen, bsz = shape_list(inp)\n",
        "\n",
        "        if mems is None:\n",
        "            mlen = self.mem_len\n",
        "        else:\n",
        "            mlen = shape_list(mems)[1]\n",
        "        klen = mlen + qlen\n",
        "\n",
        "\n",
        "        if mems is None:\n",
        "            mems = self.init_mems(bsz, mlen)\n",
        "        else:\n",
        "            mems = tf.unstack(mems, axis=0)\n",
        "            assert(shape_list(mems[0])[1] == bsz)\n",
        "            assert(len(mems) == self.n_layer)\n",
        "\n",
        "        attn_mask = self._create_mask(qlen, mlen, self.same_length)\n",
        "\n",
        "        word_emb = self.embedding_layer(inp)\n",
        "        d_word_emb = self.emb_dropout(word_emb, training=training)\n",
        "\n",
        "        pos_seq = tf.range(klen - 1, -1, -1.0)\n",
        "        if self.clamp_len > 0:\n",
        "            pos_seq = tf.minimum(pos_seq, self.clamp_len)\n",
        "        pos_emb = self.pos_emb(pos_seq)\n",
        "        d_pos_emb = self.pos_dropout(pos_emb, training=training)\n",
        "\n",
        "        core_out = d_word_emb\n",
        "        hids = []\n",
        "        for i, layer in enumerate(self.tran_layers):\n",
        "            hids.append(core_out)\n",
        "\n",
        "            mems_i = mems[i]\n",
        "            all_out = layer([core_out, d_pos_emb, attn_mask, mems_i], training=training)\n",
        "            core_out = all_out[0]\n",
        "        core_out = self.out_dropout(core_out, training=training)\n",
        "\n",
        "        new_mems = self._update_mems(hids, mems)\n",
        "        new_mems = tf.stack(new_mems)\n",
        "\n",
        "        loss = self.logsoftmax_layer([core_out, tgt], return_mean=return_mean, training=training)\n",
        "\n",
        "        # transpose loss back to shape [bsz, len] if necessary\n",
        "        if loss.shape.ndims == 2:\n",
        "            loss = tf.transpose(loss, [1, 0])\n",
        "\n",
        "        return loss, new_mems\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "apYmh5F3WwGO"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from transformers import TransfoXLLMHeadModel\n",
        "\n",
        "# Load pre-trained Transformer XL model\n",
        "model_name = 'transfo-xl-wt103'\n",
        "model = TransfoXLLMHeadModel.from_pretrained(model_name)\n",
        "\n",
        "\n",
        "# Example word2vec vectors\n",
        "word2vec_vectors = np.array([\n",
        "    [0.2, 0.4, -0.1, 0.8],\n",
        "    [0.5, -0.3, 0.7, -0.2],\n",
        "    [0.1, 0.6, -0.4, 0.9]\n",
        "])\n",
        "\n",
        "# Convert word2vec vectors to tensors\n",
        "word2vec_tensors = torch.tensor(word2vec_vectors, dtype=torch.long)\n",
        "\n",
        "\n",
        "# Generate representation using Transformer XL\n",
        "with torch.no_grad():\n",
        "    hidden_states = model.get_input_embeddings()(word2vec_tensors)\n",
        "\n",
        "# Use the hidden states for further processing or analysis\n",
        "print(hidden_states.shape)  # Print the shape of the hidden states\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "quRZ_qMFYmoA"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from transformers import TransfoXLLMHeadModel\n",
        "\n",
        "# Load pre-trained Transformer XL model\n",
        "model_name = 'transfo-xl-wt103'\n",
        "model = TransfoXLLMHeadModel.from_pretrained(model_name)\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Example word2vec vectors\n",
        "word2vec_vectors = np.array([\n",
        "    [0.2, 0.4, -0.1, 0.8],\n",
        "    [0.5, -0.3, 0.7, -0.2],\n",
        "    [0.1, 0.6, -0.4, 0.9]\n",
        "])\n",
        "\n",
        "# Convert word2vec vectors to tensors\n",
        "word2vec_tensors = torch.tensor(word2vec_vectors, dtype=torch.long).to(device)\n",
        "\n",
        "# Reshape the tensors to match the expected input shape of Transformer XL\n",
        "word2vec_tensors = word2vec_tensors.unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "# Generate representation using Transformer XL\n",
        "with torch.no_grad():\n",
        "    hidden_states = model.get_input_embeddings()(word2vec_tensors)\n",
        "\n",
        "# Use the hidden states for further processing or analysis\n",
        "print(hidden_states.shape)  # Print the shape of the hidden states\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qpTLxMNHb2SJ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import TransfoXLLMHeadModel\n",
        "\n",
        "# Load pre-trained Transformer XL model\n",
        "model_name = 'transfo-xl-wt103'\n",
        "model = TransfoXLLMHeadModel.from_pretrained(model_name)\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Example word2vec vectors\n",
        "word2vec_vectors = torch.tensor([\n",
        "    [0.2, 0.4, -0.1, 0.8],\n",
        "    [0.5, -0.3, 0.7, -0.2],\n",
        "    [0.1, 0.6, -0.4, 0.9]\n",
        "], dtype=torch.long).to(device)\n",
        "\n",
        "# Modify the size of the embedding layer\n",
        "embedding_dim = 100  # Desired embedding dimensions\n",
        "model.resize_token_embeddings(embedding_dim)\n",
        "\n",
        "# Generate representation using Transformer XL\n",
        "with torch.no_grad():\n",
        "    hidden_states = model.get_input_embeddings()(word2vec_vectors)\n",
        "\n",
        "# Use the hidden states for further processing or analysis\n",
        "print(hidden_states.shape)  # Print the shape of the hidden states\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SNwmR3eNcDNl"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import TransfoXLLMHeadModel\n",
        "\n",
        "# Load pre-trained Transformer XL model\n",
        "model_name = 'transfo-xl-wt103'\n",
        "model = TransfoXLLMHeadModel.from_pretrained(model_name)\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Example word2vec vectors\n",
        "word2vec_vectors = torch.tensor([\n",
        "    [0.2, 0.4, -0.1, 0.8],\n",
        "\n",
        "], dtype=torch.long).to(device)\n",
        "\n",
        "# Desired embedding dimensions\n",
        "embedding_dim = 100\n",
        "\n",
        "# Resize the embedding layer\n",
        "model.resize_token_embeddings(model.config.vocab_size + embedding_dim)\n",
        "\n",
        "# Generate representation using Transformer XL\n",
        "with torch.no_grad():\n",
        "    hidden_states = model.get_input_embeddings()(word2vec_vectors)\n",
        "\n",
        "# Use the hidden states for further processing or analysis\n",
        "print(hidden_states)  # Print the shape of the hidden states\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eu0S5jPRdvb1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from transformers import TransfoXLLMHeadModel\n",
        "\n",
        "\n",
        "# Load pre-trained Transformer XL model\n",
        "\n",
        "model_name = 'transfo-xl-wt103'\n",
        "model = TransfoXLLMHeadModel.from_pretrained(model_name)\n",
        "\n",
        "# Define the desired embedding size\n",
        "embedding_size = 300\n",
        "\n",
        "\n",
        "# Modify the embedding size in the configuration\n",
        "model.d_embed = embedding_size\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Example word2vec vectors\n",
        "word2vec_vectors = np.array([\n",
        "    [0.2, 0.4, -0.1, 0.8]\n",
        "])\n",
        "\n",
        "# Convert word2vec vectors to tensors\n",
        "word2vec_tensors = torch.tensor(word2vec_vectors, dtype=torch.long).to(device)\n",
        "\n",
        "\n",
        "# Generate representation using Transformer XL\n",
        "with torch.no_grad():\n",
        "    hidden_states = model.get_input_embeddings()(word2vec_tensors)\n",
        "    outputs = model(word2vec_tensors)\n",
        "\n",
        "# Get the output logits\n",
        "logits = outputs.logits\n",
        "\n",
        "# Use the logits for further processing or analysis\n",
        "print(logits[0].shape)  # Print the shape of the logits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5giqVfz-hpem"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}