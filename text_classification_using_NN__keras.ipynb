{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('ggplot')\n",
        "\n",
        "import re\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "url = 'https://raw.githubusercontent.com/SyedT1/NLP/main/csv_files/IMDB%20Dataset.csv'\n",
        "df = pd.read_csv(url)\n",
        "df.drop(columns=df.columns[2:4], inplace=True)"
      ],
      "metadata": {
        "id": "3iWjVTG1lh91"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reviews = df['review']\n",
        "df['sentiment'] = df['sentiment'].replace(['positive'], 1)\n",
        "df['sentiment'] = df['sentiment'].replace(['negative'],0)\n",
        "sentiments = df['sentiment']"
      ],
      "metadata": {
        "id": "dVfH-of8rIje"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UVvn8pWGuOq2",
        "outputId": "a4a4446a-2563-4436-a619-00dfb28604d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                              review sentiment\n",
            "0  One of the other reviewers has mentioned that ...         1\n",
            "1  A wonderful little production. <br /><br />The...         1\n",
            "2  I thought this was a wonderful way to spend ti...         1\n",
            "3  Basically there's a family where a little boy ...         0\n",
            "4  Petter Mattei's \"Love in the Time of Money\" is...         1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I7zQZwgJ2f55"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3l1C8GOu0Ptk",
        "outputId": "92c68105-b349-4e2c-ead1-19f0936b4df5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "y = np.int32(df.sentiment.astype('category').cat.codes.to_numpy())\n",
        "num_classes = np.unique(y).shape[0]\n",
        "print(len(y))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SgTU2QYbzaNJ",
        "outputId": "71dd78d4-797a-4d7c-b009-390c7f2c078b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16502\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = WordNetLemmatizer()\n",
        "def custom_standardization(text):\n",
        "    text = re.sub('<br />', ' ', str(text))\n",
        "    text = re.sub(r'\\W', ' ', str(text))\n",
        "    text = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', text)\n",
        "    # substituting multiple spaces with single space\n",
        "    text = re.sub(r'\\s+', ' ', text, flags=re.I)\n",
        "    # removing prefixed 'b'\n",
        "    text = re.sub(r'^b\\s+', '', text)\n",
        "    # converting to Lowercase\n",
        "    text = text.lower()\n",
        "    text = text.split()\n",
        "    text = [stemmer.lemmatize(word) for word in text]\n",
        "    text = ' '.join(text)\n",
        "    return text\n",
        "    pass\n",
        "df['review'] = df.review.apply(custom_standardization)"
      ],
      "metadata": {
        "id": "uazRoJ1SzyU9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_features = 10000\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from collections import Counter\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "results = Counter()\n",
        "df['review'].str.split().apply(results.update)\n",
        "vocabulary = [key[0] for key in results.most_common(max_features)]\n",
        "tokenizer = Tokenizer(num_words=max_features)\n",
        "tokenizer.fit_on_texts(vocabulary)"
      ],
      "metadata": {
        "id": "YuUEMGgiAYtx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "X = tokenizer.texts_to_sequences(df['review'].values)\n",
        "X = pad_sequences(X)\n",
        "\n",
        "\n",
        "# split and enzoy :/\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "St0vo5PB1nNf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = Counter()\n",
        "df['review'].str.split().apply(results.update)\n",
        "vocabulary = [key[0] for key in results.most_common(max_features)]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tksYS9nl3znk",
        "outputId": "319b2a2e-3020-4aa1-f481-9337af2a9e4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_dim = 16\n",
        "max_input_length = X.shape[1]\n"
      ],
      "metadata": {
        "id": "7tHs9B3f-frF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Embedding(input_dim=max_features, output_dim=output_dim, input_length=max_input_length))\n",
        "model.add(tf.keras.layers.Dropout(0.2))\n",
        "model.add(tf.keras.layers.GlobalAveragePooling1D())\n",
        "model.add(tf.keras.layers.Dense(16, activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(num_classes, activation='softmax'))"
      ],
      "metadata": {
        "id": "hJffr1X7BAWn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(), optimizer=tf.keras.optimizers.Adam(1e-4), metrics=[\"accuracy\"])\n",
        "\n",
        "\n",
        "model.summary()\n",
        "\n",
        "history_1 = model.fit(X_train, y_train, batch_size=8,epochs=20, validation_data=(X_val, y_val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07sHjxvUBev-",
        "outputId": "ac7cf134-550f-4317-ca94-e2bce146adb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_2 (Embedding)     (None, 1746, 16)          160000    \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 1746, 16)          0         \n",
            "                                                                 \n",
            " global_average_pooling1d_2   (None, 16)               0         \n",
            " (GlobalAveragePooling1D)                                        \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 16)                272       \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 16)                272       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 160,544\n",
            "Trainable params: 160,544\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "1320/1320 [==============================] - 17s 12ms/step - loss: 2.1032 - accuracy: 0.4682 - val_loss: 1.1209 - val_accuracy: 0.4869\n",
            "Epoch 2/20\n",
            "1320/1320 [==============================] - 15s 11ms/step - loss: 0.8319 - accuracy: 0.5001 - val_loss: 0.7312 - val_accuracy: 0.4869\n",
            "Epoch 3/20\n",
            "1320/1320 [==============================] - 15s 12ms/step - loss: 0.7178 - accuracy: 0.5055 - val_loss: 0.7058 - val_accuracy: 0.5127\n",
            "Epoch 4/20\n",
            "1320/1320 [==============================] - 15s 11ms/step - loss: 0.7059 - accuracy: 0.5131 - val_loss: 0.7003 - val_accuracy: 0.5479\n",
            "Epoch 5/20\n",
            "1320/1320 [==============================] - 16s 12ms/step - loss: 0.7026 - accuracy: 0.5177 - val_loss: 0.6988 - val_accuracy: 0.4869\n",
            "Epoch 6/20\n",
            "1320/1320 [==============================] - 15s 11ms/step - loss: 0.7013 - accuracy: 0.5305 - val_loss: 0.6970 - val_accuracy: 0.5127\n",
            "Epoch 7/20\n",
            "1320/1320 [==============================] - 16s 12ms/step - loss: 0.7005 - accuracy: 0.5384 - val_loss: 0.6962 - val_accuracy: 0.5134\n",
            "Epoch 8/20\n",
            "1320/1320 [==============================] - 15s 11ms/step - loss: 0.6997 - accuracy: 0.5486 - val_loss: 0.6960 - val_accuracy: 0.5941\n",
            "Epoch 9/20\n",
            "1320/1320 [==============================] - 15s 11ms/step - loss: 0.6989 - accuracy: 0.5725 - val_loss: 0.6950 - val_accuracy: 0.5127\n",
            "Epoch 10/20\n",
            "1320/1320 [==============================] - 15s 11ms/step - loss: 0.6982 - accuracy: 0.5782 - val_loss: 0.6944 - val_accuracy: 0.5150\n",
            "Epoch 11/20\n",
            "1320/1320 [==============================] - 15s 12ms/step - loss: 0.6973 - accuracy: 0.5555 - val_loss: 0.6940 - val_accuracy: 0.6842\n",
            "Epoch 12/20\n",
            "1320/1320 [==============================] - 15s 12ms/step - loss: 0.6962 - accuracy: 0.6205 - val_loss: 0.6935 - val_accuracy: 0.5127\n",
            "Epoch 13/20\n",
            "1320/1320 [==============================] - 15s 12ms/step - loss: 0.6956 - accuracy: 0.5983 - val_loss: 0.6925 - val_accuracy: 0.7739\n",
            "Epoch 14/20\n",
            "1320/1320 [==============================] - 15s 11ms/step - loss: 0.6943 - accuracy: 0.6589 - val_loss: 0.6912 - val_accuracy: 0.5256\n",
            "Epoch 15/20\n",
            "1320/1320 [==============================] - 15s 12ms/step - loss: 0.6927 - accuracy: 0.6043 - val_loss: 0.6901 - val_accuracy: 0.6327\n",
            "Epoch 16/20\n",
            "1320/1320 [==============================] - 15s 11ms/step - loss: 0.6913 - accuracy: 0.6837 - val_loss: 0.6893 - val_accuracy: 0.6978\n",
            "Epoch 17/20\n",
            "1320/1320 [==============================] - 15s 11ms/step - loss: 0.6896 - accuracy: 0.6753 - val_loss: 0.6874 - val_accuracy: 0.7721\n",
            "Epoch 18/20\n",
            "1320/1320 [==============================] - 15s 11ms/step - loss: 0.6873 - accuracy: 0.6732 - val_loss: 0.6877 - val_accuracy: 0.5494\n",
            "Epoch 19/20\n",
            "1320/1320 [==============================] - 16s 12ms/step - loss: 0.6854 - accuracy: 0.7041 - val_loss: 0.6840 - val_accuracy: 0.7334\n",
            "Epoch 20/20\n",
            "1320/1320 [==============================] - 16s 12ms/step - loss: 0.6829 - accuracy: 0.7399 - val_loss: 0.6813 - val_accuracy: 0.6361\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "probabilities = model.predict(X_test)\n",
        "pred = np.argmax(probabilities, axis=1)\n",
        "\n",
        "print(\" \")\n",
        "print(\"Results\")\n",
        "\n",
        "accuracy = accuracy_score(y_test, pred)\n",
        "\n",
        "print('Shothik koddur: {:.6f}'.format(accuracy))\n",
        "print(\" \")\n",
        "print(classification_report(y_test, pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "id": "HbWNMS45C4pg",
        "outputId": "120420d9-4111-4c9a-9088-fb017fc92754"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-4a07bc5f2a9a>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprobabilities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobabilities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Results\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    }
  ]
}